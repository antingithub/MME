{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import os, sys\n",
    "import re\n",
    "import calendar\n",
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.style.use('_classic_test')\n",
    "# plt.style.use('seaborn-paper')\n",
    "# plt.style.use('seaborn-pastel')\n",
    "# plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configurations.yaml', 'r') as stream:\n",
    "    CONFIG = yaml.load(stream, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset_path = CONFIG['observations']['temperature']['dataset_path']\n",
    "temp_output_path = CONFIG['observations']['temperature']['output_path']\n",
    "prec_dataset_path = CONFIG['observations']['percepitation']['dataset_path']\n",
    "prec_output_path = CONFIG['observations']['percepitation']['output_path']\n",
    "\n",
    "models_dataset_path = CONFIG['models']['dataset_path']\n",
    "models_output_path = CONFIG['models']['output_path']\n",
    "\n",
    "plots_output_path = CONFIG['plots']['output']\n",
    "\n",
    "models_start_year = CONFIG['models']['time']['start']['year']\n",
    "models_hour_interval = CONFIG['models']['time']['hour_interval']\n",
    "models_start_timestamp = CONFIG['models']['time']['timestamp']['start']\n",
    "models_end_timestamp = CONFIG['models']['time']['timestamp']['end']\n",
    "models_freq_timestamp = CONFIG['models']['time']['timestamp']['freq']\n",
    "\n",
    "models_temp_filename = CONFIG['models']['variables']['temperature']['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_dict = CONFIG['dates']['seasons_dict']\n",
    "period = CONFIG['period']['historical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE!\n",
    "# REDO\n",
    "\n",
    "\n",
    "def get_xls_files(variable, obs_path):\n",
    "    '''\n",
    "        variable: 'temp' or 'prec'\n",
    "        glob_path: path to file type excel, used by glob\n",
    "            example: '..\\\\data\\\\observations\\\\*.xls'\n",
    "            \n",
    "        Returns a dict of all file_paths in the following format\n",
    "            file_path': (variable, year)\n",
    "            item example: '..\\\\data\\\\observations\\\\PREC1979.XLS': ('prec', '1979')\n",
    "\n",
    "    '''\n",
    "    glob_path = os.path.join(os.getcwd(), obs_path) + '\\*.xls'    # the '\\'  in '\\*.xls' is nedded\n",
    "    print(glob_path)\n",
    "    \n",
    "    def all_4digits_year(dict_files):\n",
    "        '''\n",
    "            returns the dict of files with porper year yyyy\n",
    "        '''\n",
    "        return {file:(pair[0], '19'+ pair[1] if len(pair[1]) == 2 else pair[1]) for file,pair in dict_files.items()}\n",
    "\n",
    "    file_pattern = re.compile(rf'{variable}\\s*(?P<year>\\d*).xls' , flags=re.IGNORECASE)\n",
    "    files = {f:(variable, re.search(file_pattern, os.path.basename(f)).group(1)) for f in glob(glob_path) if re.match(file_pattern, os.path.basename(f))}\n",
    "    return all_4digits_year(files)\n",
    "\n",
    "\n",
    "def get_data_from_xls(filename_path, year):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    xls = pd.ExcelFile(filename_path)\n",
    "    df_raw = pd.read_excel(xls, header=None, usecols=range(0,25), index=False)\n",
    "    # clean rows with no data (first col empty, must have a day or a month)\n",
    "    df_raw.dropna(axis=0, how='all', subset=[0], inplace=True)\n",
    "\n",
    "    # df to hold final result\n",
    "    columns = ['month', 'day', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    df_temp = pd.DataFrame(columns = columns)\n",
    "\n",
    "    months_list = ['jan', 'fev', 'mar', 'abr', 'mai', 'jun', 'jul', 'ago', 'set', 'out', 'nov', 'dez', 'stop']\n",
    "    months_dict = {m:i for i,m in enumerate(months_list[:-1], 1)}\n",
    "    def month_iter(months):     # allows to consume all the months once, it uses a 'stop' word to easing the parser\n",
    "        for month in months:\n",
    "            yield month\n",
    "\n",
    "    # regex for 1-31\n",
    "    day_pattern = re.compile(r'(?P<day>([1-9]|[12]\\d|3[01]))$') \n",
    "\n",
    "    # month iterator (can be done with a simple list as well)\n",
    "    month_it = month_iter(months_list)\n",
    "    month = next(month_it) # starts with 'jan'\n",
    "\n",
    "    for row in df_raw.iterrows():\n",
    "        label = row[1][0]                                                  # for each row the first col\n",
    "        month_pattern = re.compile(rf'{month}' , flags=re.IGNORECASE)\n",
    "        if re.match(month_pattern, str(label)):                      # assumes that the first line has the 'jan' matching pattern\n",
    "            prev_month = month\n",
    "            try:\n",
    "                month = next(month_it)\n",
    "            except:\n",
    "                break                                               # the end of the list\n",
    "        if re.match(day_pattern, str(label)):\n",
    "            day = label                                             # it is a day\n",
    "            new_series = pd.Series([months_dict[prev_month], day]).append(row[1][1:]) # select only the values (temp/prec)\n",
    "            new_series.index = df_temp.columns\n",
    "            df_temp = df_temp.append(new_series, ignore_index=True)\n",
    "    df_temp['year'] = year      \n",
    "    return df_temp[['year'] + columns]\n",
    "\n",
    "\n",
    "\n",
    "# def get_all_obs_data_old(dict_files, variable, out_dir = None):\n",
    "#     '''\n",
    "#         Returns a df with all temperature series\n",
    "#         Saves the df to  ..\\\\data\\\\observations\\\\output\\\\\n",
    "#     '''\n",
    "#     # order file paths by year, ascending.\n",
    "#     files_years = sorted([(path,year[1]) for path,year in dict_files.items()], key = lambda v: v[1])\n",
    "\n",
    "#     columns = columns = ['year', 'month', 'day', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "#     df_data = pd.DataFrame(columns = columns)\n",
    "\n",
    "#     for file_path,year in files_years:\n",
    "#         df_temp = get_data_from_xls(file_path, year)\n",
    "#         assert (df_temp.shape[0] <= 366) and (df_temp.shape[0] >= 365), '365 or 366 days in file'\n",
    "#         print('file_path - {}\\tyear - {}\\tlen: {}'.format(file_path, year, df_temp.shape[0]))\n",
    "#         df_data = df_data.append(df_temp)\n",
    "\n",
    "#     # fix dtypes    \n",
    "#     dict_types = {col:'float' for col in df_data.columns}\n",
    "#     dict_types['year'], dict_types['month'], dict_types['day']  = 'int', 'int', 'int'\n",
    "#     df_data = df_data.astype(dict_types)\n",
    "    \n",
    "#     # save dataframe to csv\n",
    "#     filename = '{}_{}_{}.csv'.format(variable, files_years[0][1], files_years[-1][1])\n",
    "#     save_df2csv(df_data, filename, out_dir)\n",
    "#     return df_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_obs_data(dict_files, variable, out_dir = None):\n",
    "    '''\n",
    "        Returns a df with all temperature series\n",
    "        Numbers are rounded to 2 decimal places\n",
    "        Saves the df to  ..\\\\data\\\\observations\\\\output\\\\\n",
    "    '''\n",
    "    # order file paths by year, ascending.\n",
    "    files_years = sorted([(path,year[1]) for path,year in dict_files.items()], key = lambda v: v[1])\n",
    "\n",
    "    columns = ['year', 'month', 'day', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    df_data = pd.DataFrame(columns = columns)\n",
    "\n",
    "    for file_path,year in files_years:\n",
    "        df_temp = get_data_from_xls(file_path, year)\n",
    "        assert (df_temp.shape[0] <= 366) and (df_temp.shape[0] >= 365), '365 or 366 days in file'\n",
    "        print('file_path - {}\\tyear - {}\\tlen: {}'.format(file_path, year, df_temp.shape[0]))\n",
    "        df_data = df_data.append(df_temp)\n",
    "\n",
    "    # fix dtypes    \n",
    "    dict_types = {col:'float' for col in df_data.columns}\n",
    "    dict_types['year'], dict_types['month'], dict_types['day']  = 'int', 'int', 'int'\n",
    "    df_data = df_data.astype(dict_types)\n",
    "    # round to 2 decimal places\n",
    "    df_data = df_data.round(decimals = {col:2 for col in columns[3:]})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # save dataframe to csv\n",
    "    filename = '{}_{}_{}.csv'.format(variable, files_years[0][1], files_years[-1][1])\n",
    "    save_df2csv(df_data, filename, out_dir)\n",
    "    return df_data.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dates4models(ndata, year_ini = models_start_year,\n",
    "#                       hours_interval = models_hour_interval):\n",
    "#     '''\n",
    "#         ndata: number of data points\n",
    "#         year_ini = 1971\n",
    "#         hours_interval = 3\n",
    "#     '''\n",
    "#     year, month, day, hour = year_ini, 1, 1, hours_interval          # first hour is 3 (data is every 3 hours)\n",
    "#     DataV=[]\n",
    "#     for t in range(0, ndata):\n",
    "#         DataV.append([year,month,day,hour])\n",
    "#         hour  += 3\n",
    "#         if hour > 21:\n",
    "#             hour = 0\n",
    "#             day += 1\n",
    "#             if day > calendar.monthrange(year, month)[1]:\n",
    "#                 day = 1\n",
    "#                 month += 1\n",
    "#                 if month > 12:\n",
    "#                     month = 1\n",
    "#                     year += 1\n",
    "#     DataV = [[year_ini,    1,    1,    0]] + DataV\n",
    "#     return np.array(DataV)\n",
    "\n",
    "# #assert make_dates4models(102271).shape == (102272, 4)\n",
    "# # d = make_dates4models(102271)\n",
    "# # d[:10], d[-10:]\n",
    "# # array([[1971,    1,    1,    0],\n",
    "# #         [1971,    1,    1,    3],\n",
    "# #         [1971,    1,    1,    6],\n",
    "# #         [1971,    1,    1,    9],\n",
    "# #         [1971,    1,    1,   12],\n",
    "# #         [1971,    1,    1,   15],\n",
    "# #         [1971,    1,    1,   18],\n",
    "# #         [1971,    1,    1,   21],\n",
    "# #         [1971,    1,    2,    0],\n",
    "# #         [1971,    1,    2,    3]\n",
    "# #       ...\n",
    "# #         [2005,   12,   30,   18],\n",
    "# #         [2005,   12,   30,   21],\n",
    "# #         [2005,   12,   31,    0],\n",
    "# #         [2005,   12,   31,    3],\n",
    "# #         [2005,   12,   31,    6],\n",
    "# #         [2005,   12,   31,    9],\n",
    "# #         [2005,   12,   31,   12],\n",
    "# #         [2005,   12,   31,   15],\n",
    "# #         [2005,   12,   31,   18],\n",
    "# #         [2005,   12,   31,   21]])\n",
    "\n",
    "\n",
    "def select_models_folders(dir_models = models_dataset_path, period = period):\n",
    "    folders = [folder for folder in  os.listdir(os.path.join(os.getcwd(), dir_models)) if period in folder]\n",
    "    return folders, len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season2df(df):\n",
    "    cols = df.columns\n",
    "    conditions = [(df['month'].isin([12, 1, 2])), (df['month'].isin([3, 4, 5])),\n",
    "                (df['month'].isin([6, 7, 8])), (df['month'].isin([9, 10, 11]))]\n",
    "    df['season'] = np.select(conditions, seasons_dict.keys())\n",
    "    cols = cols.insert(1, 'season')\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_models(dir_models = models_dataset_path, \n",
    "                    var_filename = models_temp_filename, \n",
    "                    start_timestamp = models_start_timestamp, end_timestamp = models_end_timestamp,\n",
    "                    freq_timestamp = models_freq_timestamp, plot = False):\n",
    "    '''\n",
    "        For temperature only!! Convertion to C\n",
    "        \n",
    "    '''\n",
    "    res = []                        # to accumulate the dataseries\n",
    "    color = cycle('rbgkmc')\n",
    "    folders, nmodels = select_models_folders(dir_models, period)\n",
    "    for model_id, folder in enumerate(folders):\n",
    "        T2m = np.loadtxt(os.path.join(dir_models, folder, var_filename)) - 273.15\n",
    "        T2m = pd.Series(T2m, name = model_id, index=pd.date_range(\n",
    "                                                    start = start_timestamp,\n",
    "                                                    end = models_end_timestamp,\n",
    "                                                    freq = freq_timestamp,\n",
    "                                                    dayfirst=True ))\n",
    "        if plot:\n",
    "            T2m.plot(figsize=(18,8), color=next(color))\n",
    "            plt.title('Model: {}'.format(model_id))\n",
    "            plt.show();\n",
    "        res.append(T2m) \n",
    "        \n",
    "    res = pd.concat(res, axis = 1)\n",
    "    res.index.name = 'date'\n",
    "    \n",
    "    if plot:\n",
    "        res.plot(figsize=(18,8))\n",
    "        plt.tight_layout()\n",
    "        plt.show();\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datafile(save_fn, filename, output_dir):\n",
    "    '''\n",
    "        out_filename: defines what is computed, ex. temp_sea_avg_obs\n",
    "    '''\n",
    "    out_dir = os.path.join(os.getcwd(), output_dir)\n",
    "    if not os.path.exists(out_dir):\n",
    "        print('Output folder does not exist. Created a new one: {}'.format(out_dir))\n",
    "        os.makedirs(out_dir)\n",
    "    if filename[-4:] != '.csv':\n",
    "        filename = filename + '.csv'\n",
    "    save_fn(path_or_buf = os.path.join(os.getcwd(), out_dir, filename))\n",
    "    \n",
    "# def save_df2csv_old(df, filename, output_dir, index=False):\n",
    "#     '''\n",
    "        \n",
    "#     '''\n",
    "#     # to get other args when called\n",
    "#     save_fn = partial(df.to_csv, sep=',', header=True, index=index, date_format='%Y-%m-%d %H')\n",
    "#     return save_datafile(save_fn, filename, output_dir)\n",
    "\n",
    "def save_df2csv(df, filename, output_dir, index=False):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    # to get other args when called\n",
    "    save_fn = partial(df.to_csv, sep=',', header=True, index=index, date_format='%Y-%m-%d %H:%M:%S')\n",
    "    return save_datafile(save_fn, filename, output_dir)\n",
    "\n",
    "\n",
    "# def save_nparray2npy(data, filename, output_dir = None):\n",
    "#     if not output_dir:\n",
    "#         print('arg: output_dir is necessary.\\nTry one of these:\\n{}\\n{}'.format(temp_output_path, prec_output_path, models_output_path))\n",
    "#     save_fn = partial(save_nparray, array_data = data)\n",
    "#     return save_datafile(save_fn, filename, output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_nparray(array_data = None, path_or_buf = None):\n",
    "#     path_or_buf = path_or_buf +'.npy'\n",
    "#     with open(path_or_buf, 'wb') as f:\n",
    "#         np.save(f, array_data)\n",
    "#         print('\\nNumpy array saved to file: {}'.format(path_or_buf))\n",
    "\n",
    "\n",
    "\n",
    "def save_plot(plot, filename, output_dir = plots_output_path):\n",
    "    '''\n",
    "        out_filename: defines what is computed, ex. temp_sea_avg_obs\n",
    "    '''\n",
    "    out_dir = os.path.join(os.getcwd(), output_dir)\n",
    "    if not os.path.exists(out_dir):\n",
    "        print('Output folder does not exist. Created a new one: {}'.format(out_dir))\n",
    "        os.makedirs(out_dir)\n",
    "    path_or_buf = os.path.join(os.getcwd(), out_dir, filename)\n",
    "    plot.savefig(path_or_buf,\n",
    "                 dpi=None,\n",
    "                 facecolor='w',\n",
    "                 edgecolor='w',\n",
    "                 orientation='portrait',\n",
    "                 papertype=None, format=None,\n",
    "                 transparent=False,\n",
    "                 bbox_inches=None,\n",
    "                 pad_inches=0.1,\n",
    "                 frameon=None,\n",
    "                 metadata=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load np array\n",
    "# def load_nparray_file(in_filename_path):\n",
    "#     with open(in_filename_path, 'rb') as f:\n",
    "#         data = np.load(f)\n",
    "#     return data\n",
    "\n",
    "# def load_nparray_models(in_filename):\n",
    "#     in_dir = os.path.join(os.getcwd(), models_output_path)\n",
    "#     return load_nparray_file(os.path.join(in_dir, in_filename))\n",
    "\n",
    "\n",
    "\n",
    "# TO DELETE!!!!!\n",
    "def load_csv2df_old(filename, filepath):\n",
    "    def conv_to_int(c):    # to guarantee that the hours' column names are type int\n",
    "        try:\n",
    "            c = c.astype(int32)\n",
    "        except:\n",
    "            return c\n",
    "        return c\n",
    "    date_dtype = {'year':np.int32, 'month':np.int32, 'day':np.int32}\n",
    "    filename_path = os.path.join(os.getcwd(), filepath, filename)\n",
    "    df = pd.read_csv(filename_path, sep=',', parse_dates=True, dtypes = hours_dtype)\n",
    "    df.columns = [conv_to_int(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def load_csv2df(filename, filepath, freq_index=None):\n",
    "    '''\n",
    "        if it is a timeseries 'date' and a 'freq_index' value is passed it is the index\n",
    "    '''\n",
    "    def conv_to_int(c):    # to guarantee that the hours' column names are type int\n",
    "        try:\n",
    "            c = int(c)\n",
    "        except:\n",
    "            return c\n",
    "        return c\n",
    "    \n",
    "    filename_path = os.path.join(os.getcwd(), filepath, filename)\n",
    "    date_dtype = {'year':np.int32, 'month':np.int32, 'day':np.int32}\n",
    "    df = pd.read_csv(filename_path, sep=',', parse_dates=True, dtype = date_dtype)\n",
    "    \n",
    "    df.columns = [conv_to_int(col) for col in df.columns]\n",
    "    if ('date' in df.columns) and (freq_index):\n",
    "        df.set_index('date', inplace=True) # if it is a timeseries 'date' is the index\n",
    "        df.index = pd.DatetimeIndex(df.index.values, freq=freq_index)\n",
    "        df.index.name = 'date'\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def compare_saved_loaded_df(df_, df):\n",
    "    assert df_.index.equals(df.index)\n",
    "    assert df_.columns.equals(df.columns)\n",
    "    assert np.isclose(df_.values, df.values, equal_nan=True).all()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIMIZE!\n",
    "\n",
    "# def obs_avg_3h(df):\n",
    "#     df_obs = df.iloc[:, 0:3]  # use Dates\n",
    "#     for c in range(0, 22, 3):\n",
    "#         df_obs[c] = df.iloc[:, c+3:c+3+3].mean(axis=1)\n",
    "#     return df_obs.round(decimals=2)                              # decimals=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_timeseries_format(df, existing_cols, new_col_name, var_name):\n",
    "#     '''\n",
    "#         df\n",
    "#         existing_cols: list\n",
    "#         new_col_name: string\n",
    "#         var_name:string\n",
    "#         Example:header = [year\tmonth\tday\t0\t3\t6\t9\t12\t15\t18\t21]\n",
    "#                 new_header = [year\tmonth\tday\thour\ttemp_3h]\n",
    "#     '''\n",
    "#     df_temp = add_season2df(df_temp)\n",
    "#     df_temp = pd.melt(df, id_vars=existing_cols, var_name=new_col_name, value_name=var_name)\n",
    "#     # 'hour' must be int\n",
    "#     df_temp['hour'] = df_temp['hour'].astype(int)\n",
    "#     df_temp.reset_index(drop=True, inplace=True)\n",
    "#     print(df_obs.iloc[327634:327636,:])\n",
    "#     df_temp['date'] = df_temp.apply(lambda s: pd.datetime(*map(int, (s.year, s.month, s.day, s.hour)))\n",
    "#                                   , axis=1)\n",
    "#     df_temp = df_temp.set_index('date')\n",
    "#     df_temp.drop(['year','month','day','hour'], axis=1, inplace=True)\n",
    "#     df_temp.sort_index(inplace=True)\n",
    "#     return df_temp\n",
    "\n",
    "\n",
    "#### TO DELETE\n",
    "# def obs_ts_format_old(df, existing_cols, new_col_name, var_name):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     df_temp = df.copy(deep=True)\n",
    "#     df_temp = pd.melt(df_temp, id_vars=existing_cols, var_name=new_col_name, value_name=var_name)\n",
    "\n",
    "#     df_temp = add_season2df(df_temp)                  # add season column\n",
    "#     df_temp['hour'] = df_temp['hour'].astype(int)     # 'hour' must be int\n",
    "#     df_temp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     df_temp['date'] = df_temp.apply(lambda s: pd.datetime(*map(int, (s.year, s.month, s.day, s.hour)))\n",
    "#                                   , axis=1)\n",
    "#     df_temp = df_temp.set_index('date')\n",
    "#     df_temp.drop(['year','month','day','hour'], axis=1, inplace=True)\n",
    "#     df_temp.sort_index(inplace=True)\n",
    "#     return df_temp\n",
    "\n",
    "\n",
    "def obs_ts_format(df, existing_cols, new_col_name, var_name, freq_index='3H'):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df_temp = df.copy(deep=True)\n",
    "    df_temp = pd.melt(df_temp, id_vars=existing_cols, var_name=new_col_name, value_name=var_name)\n",
    "\n",
    "    df_temp = add_season2df(df_temp)                  # add season column\n",
    "    df_temp['hour'] = df_temp['hour'].astype(int)     # 'hour' must be int\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_temp['date'] = df_temp.apply(lambda s: pd.datetime(*map(int, (s.year, s.month, s.day, s.hour)))\n",
    "                                  , axis=1)\n",
    "    df_temp = df_temp.set_index('date')\n",
    "    df_temp.drop(['year','month','day','hour'], axis=1, inplace=True)\n",
    "    df_temp.sort_index(inplace=True)\n",
    "    df_temp = df_temp.asfreq(freq_index)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_season(df, season = None, month_col='month'):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    return df.loc[df['month'].isin(seasons_dict[season]), :]\n",
    "\n",
    "# def filter_df_by_hour_old(df, hour = None, hour_col='hour'):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     df = df.copy()\n",
    "#     return df.loc[df['hour'] == hour, :]\n",
    "\n",
    "\n",
    "def filter_ts_by_hour(df, hour = None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    return df[df.index.hour==hour]\n",
    "\n",
    "\n",
    "\n",
    "def filter_ts_by_season(ts, season = None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    ts = ts.copy()\n",
    "    return ts.loc[ts.index.month.isin(seasons_dict[season]), :]\n",
    "\n",
    "\n",
    "def df_all_models(list_df_models):\n",
    "    cols = list_df_models[0].columns\n",
    "    for cmod,model in enumerate(list_df_models):\n",
    "        model['model'] = cmod\n",
    "    df =  pd.concat([*list_df_models], axis=0)\n",
    "    return df[['model', *cols]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def join_obs2models(df_obs = None, df_models = None,\n",
    "#                     obs_file = None, models_file = None, \n",
    "#                     obs_path = None, models_path = None):\n",
    "#     '''\n",
    "#         Prepare data for computation of differences between observations and models\n",
    "#         For both dataframes and csv files the data must be \"by_hour\" kind!\n",
    "    \n",
    "#     '''\n",
    "#     # get observations and model dataframes\n",
    "#     if not isinstance(df_obs, pd.DataFrame):\n",
    "#         try:\n",
    "#             df_obs = load_csv2df(obs_file, obs_path if obs_path else temp_output_path)\n",
    "#         except:\n",
    "#             print('You must specify a valid observations filename and path')\n",
    "#     if not isinstance(df_models, pd.DataFrame):\n",
    "#         try:\n",
    "#             df_models = load_csv2df(models_file, models_path if models_path else models_output_path)\n",
    "#         except:\n",
    "#             print('You must specify a valid models filename and path')\n",
    "    \n",
    "#     # prepare observations df\n",
    "#     df_obs = add_season2df(df_obs)\n",
    "#     ts_obs = make_timeseries_format(df_obs, ['year', 'season', 'month', 'day'],\n",
    "#                            'hour', 17)\n",
    "#     key = ['year','season', 'month', 'day', 'hour']\n",
    "#     ts_obs = ts_obs.set_index(key)\n",
    "    \n",
    "#     # prepare models dataframe\n",
    "#     # columns 'mod_obs' will hold the models' numbers and the observation number = 17\n",
    "#     df_models = df_models.rename(columns = {'model':'mod_obs'})\n",
    "#     # all variables in 'index' and only temperature as column/time series\n",
    "#     ts_models = make_timeseries_format(df_models, ['mod_obs', 'year', 'season', 'month', 'day'],\n",
    "#                            'hour', 'temp_3h')\n",
    "#     # transform table to put models in column\n",
    "#     df_by_model = ts_models.pivot_table(values='temp_3h', columns='mod_obs', index=key)\n",
    "#     # joinning\n",
    "#     df_by_model = df_by_model.join(ts_obs,  lsuffix='L', rsuffix='R', how='inner')\n",
    "#     df_by_model.sort_index(inplace=True)\n",
    "#     df_by_model.reset_index(inplace=True)\n",
    "#     return df_by_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentiles_table(df, values, by = 'hour', percentiles = [1,5,25,50,75,95,99]):\n",
    "    '''\n",
    "        Returns the percentiles of a value by a specific column (by)\n",
    "        Organizes a timeseries in tabular form for a specific column\n",
    "        df header: year\tmonth\tday\thour\ttemp_3h\n",
    "        \n",
    "        values: example 'temp_3h'\n",
    "        \n",
    "    '''\n",
    "    index = [col for col in df.columns if col not in [values, by]]\n",
    "    df  = df_obs_3_jja.pivot_table(values = values, columns = by, index = index)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_desc = df.describe(percentiles=np.asarray(percentiles)/100)\n",
    "    df_desc = df_desc.T\n",
    "    df_desc.reset_index(inplace=True)\n",
    "    df_desc.rename(columns={'index': by}, inplace=True)\n",
    "    df_desc.drop(['count', 'std'], axis=1, inplace=True)\n",
    "    df_desc\n",
    "    return df_desc.set_index(by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_models_obs(metric, df_data, model_cols, observation_col):\n",
    "    return df_data[model_cols].apply(metric, obs = df_data[observation_col], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_(s, obs):\n",
    "    return (obs - s).mean()\n",
    "\n",
    "  \n",
    "def bias(df, model_cols, observation_col):\n",
    "    return fn_models_obs(diff_, df, model_cols, observation_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### S score - Perkins Skill score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(series, bins = None):\n",
    "    '''\n",
    "        pdf for pandas dataSeries, using a numpy array func\n",
    "        returns both the density and the bin edges\n",
    "    '''\n",
    "    density, _ = np.histogram(series, bins = bins, density=True)\n",
    "    return density\n",
    "\n",
    "\n",
    "def  minimun(s, obs):\n",
    "    return np.minimum(s, obs)\n",
    "\n",
    "def perkins_skill_score(df, model_cols, obs_col, bins=None):\n",
    "    if not bins:\n",
    "        bins = range(0, 51)   # 0ºC to 50ºC\n",
    "    list_pdfs = [pdf(s, bins) for _,s in df[model_cols+[obs_col]].items()]\n",
    "    # transpose it!\n",
    "    df_pdf = pd.DataFrame(list_pdfs, index=list(range(0,18)), columns=bins[:-1]).T\n",
    "\n",
    "    return fn_models_obs(minimun, df_pdf, model_cols, obs_col).sum(axis=0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigma score - Normalized standard deviation measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_(s, obs):\n",
    "    return s.std()/obs.std()\n",
    "\n",
    "\n",
    "def sigma_score(df, model_cols, obs_col):\n",
    "    return fn_models_obs(sigma_, df, model_cols, obs_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YK skewness - Yule-Kendall skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YK_old(df, model_cols, obs_col):\n",
    "    df = df[model_cols+[obs_col]].describe(percentiles=[0.05, 0.5, 0.95]).iloc[4:7, :]\n",
    "    return ((df.loc['95%', :] - df.loc['50%', :]) - (df.loc['50%', :] - df.loc['5%', :]))/ (df.loc['95%', :] - df.loc['5%', :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YK_(df, model_cols, obs_col):\n",
    "    df = df[model_cols+[obs_col]].describe(percentiles=[0.05, 0.5, 0.95]).iloc[4:7, :]\n",
    "    return ((df.loc['95%', :] - df.loc['50%', :]) - (df.loc['50%', :] - df.loc['5%', :]))/ (df.loc['95%', :] - df.loc['5%', :])\n",
    "\n",
    "\n",
    "def yk_diff(s, obs):\n",
    "    return s-obs\n",
    "\n",
    "def YK_skewness_by_hour(df, model_cols, obs_col):\n",
    "    df_yk = metric_by_hour(YK_, df, model_cols, obs_col)\n",
    "    return df_yk.loc[MODELS_NUMBERS, :].apply(yk_diff, obs = df_yk.loc[OBS_NUMBER, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_by_hour(metric, df, model_cols, obs_col):\n",
    "    res = []\n",
    "    for hour in range(0,22,3):\n",
    "        df_filtered = filter_ts_by_hour(df, hour)\n",
    "        res.append(metric(df_filtered, model_cols, obs_col))\n",
    "    return pd.concat(res, axis=1, keys=list(range(0,22,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pdf, cdf, pmf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdf_cdf(sample, plot=True):\n",
    "    '''\n",
    "        Visualize the ECDF, interpolated PDF and samples\n",
    "        \n",
    "    '''\n",
    "    bins=int(np.sqrt(len(sample)))\n",
    "#     bins =  100\n",
    "    hist = np.histogram(sample, bins = bins)\n",
    "    hist_dist = st.rv_histogram(hist)\n",
    "\n",
    "    sample_min, sample_max = np.min(sample), np.max(sample)\n",
    "    pdf, Ecdf = hist_dist.pdf, hist_dist.cdf\n",
    "    assert Ecdf(sample_min) == 0, 'Something wrong with ECDF!'\n",
    "    assert Ecdf(sample_max) == 1, 'Something wrong with ECDF!'\n",
    "\n",
    "    if plot:\n",
    "        X = np.linspace(sample_min, sample_max, bins)\n",
    "        plt.title(\"PDF\")\n",
    "        _ = plt.hist(sample, density = True, bins = bins)\n",
    "        _ = plt.plot(X, pdf(X), label = 'PDF')\n",
    "        #_ = plt.plot(X, Ecdf(X), label = 'CDF')\n",
    "        plt.legend()\n",
    "        plt.margins(0.02)\n",
    "        plt.show()\n",
    "    return pdf, Ecdf\n",
    "\n",
    "\n",
    "def compute_pdf_KDE(sample, bandwidth=2, kernel='gaussian', plot=True):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    bins=int(np.sqrt(len(sample)))\n",
    "    #print(bins)\n",
    "    sample = sample.reshape((len(sample), 1))\n",
    "    # fit density\n",
    "    model = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n",
    "    model.fit(sample)\n",
    "    \n",
    "    if plot:\n",
    "        sample_min, sample_max = np.min(sample), np.max(sample)\n",
    "        X = np.linspace(sample_min, sample_max, bins)\n",
    "        values = X\n",
    "        values = values.reshape((len(values), 1))\n",
    "        #print(values.shape)\n",
    "        probabilities = model.score_samples(values)\n",
    "        probabilities = np.exp(probabilities)\n",
    "        _ = plt.hist(sample, bins=bins, density=True)\n",
    "        _ = plt.plot(values[:], probabilities)\n",
    "        plt.title(\"pdf using KDE\")\n",
    "        plt.show()\n",
    "    #return np.exp(model.score_samples)\n",
    "    return model.score_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_season_avg_temp(data, hours_interval = models_hour_interval):\n",
    "#     hours = np.arange(0, 22, 3)\n",
    "\n",
    "#     fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10), sharex=True, sharey=True)\n",
    "#     fig.suptitle('Temperature Season Average')\n",
    "\n",
    "#     for nsea,sea,ax in zip(range(4), seasons_dict.keys(), axs.flat):\n",
    "#         for cmod in range(0,17):\n",
    "#             ax.plot(hours, data[cmod, nsea, :], ':', alpha=0.6)\n",
    "#             ax.set_title(sea)\n",
    "#         ax.plot(hours, np.mean(data[:, nsea, :], axis=0) , '-')\n",
    "\n",
    "#     fig.legend(labels = [i for i in range(17)] + ['avg'], loc='upper right')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metrics_by_hour(df, title,\n",
    "                        filename = None,\n",
    "                        output_dir = plots_output_path,\n",
    "                        sub_folder=None,\n",
    "                        df_other=None):\n",
    "    # fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 30), sharex=True, sharey=True)\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 10), sharex=True, sharey=True)  # figsize=(w, h)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    \n",
    "    for hour,ax in zip(range(0,22,3), axs.flat):\n",
    "        plt.subplot(ax)\n",
    "        ax.plot(df[hour], '-o')\n",
    "        if isinstance(df_other, pd.DataFrame):\n",
    "            plt.plot(df_other[hour], '-o')\n",
    "        ax.set_title('Hour = {}'.format(hour), fontsize=14)\n",
    "        plt.grid(axis='x', color='gray', linestyle='--')\n",
    "        plt.xticks(list(range(0,17)))\n",
    "        ax.margins(0.05)\n",
    "    if isinstance(df_other, pd.DataFrame):\n",
    "        plt.legend(labels=['bias corrected'], loc='best')\n",
    "    if filename:\n",
    "        if sub_folder:\n",
    "            output_dir = os.path.join(output_dir, sub_folder)\n",
    "        save_plot(plt, filename+'_per_h', output_dir = output_dir)\n",
    "    plt.show();\n",
    "    \n",
    "    df.plot(figsize=(15, 10), style = '-o')\n",
    "    fig.legend(labels = df.columns, loc='upper right', title='hour')\n",
    "    plt.xticks(list(range(0,17)))\n",
    "    plt.margins(0.05)\n",
    "    plt.grid(axis='x', color='silver', linestyle='--')\n",
    "    if filename:\n",
    "        save_plot(plt, filename, output_dir = output_dir)\n",
    "    print(output_dir)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ranks(df_metric, metric=None, scale=None, decimals=None): \n",
    "    if metric == 'bias':\n",
    "        scale = 7\n",
    "        metric_prep = df_metric.abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    elif metric == 'perkins':\n",
    "        scale = 100\n",
    "        metric_prep = df_metric.div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=True)\n",
    "    \n",
    "    elif metric == 'sigma':\n",
    "        scale = 2\n",
    "        metric_prep = (df_metric - 1).abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    elif metric == 'yk':\n",
    "        scale = 1\n",
    "        metric_prep = df_metric.abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    else:\n",
    "        print('{} is not a valid metric [bias, sigma, perkins, yk]'.format(metric))\n",
    "    \n",
    "    print('abs({})\\tmin:{}\\tmax:{} '.format(metric, df_metric.abs().min().min(), df_metric.abs().max().max()))\n",
    "    print('transf({})\\tmin:{}\\tmax:{} '.format(metric, metric_prep.abs().min().min(), metric_prep.abs().max().max()))\n",
    "    print('{} rank\\tmin:{}\\tmax:{} '.format(metric, metric_rank.abs().min().min(), metric_rank.abs().max().max()))\n",
    "    \n",
    "    return metric_rank\n",
    "\n",
    "\n",
    "def plot_bump(metric_rank,\n",
    "              title,\n",
    "              filename=None,\n",
    "              output_dir = plots_output_path,\n",
    "              sub_folder=None, \n",
    "              figsize=(18,18)):\n",
    "    for row in metric_rank.iterrows():\n",
    "        delta = row[0]*0.025\n",
    "        metric_rank.iloc[row[0],:] = row[1] + delta\n",
    "\n",
    "    metric_rank.T.plot(figsize=(18,18), style='-o')\n",
    "    plt.yticks(list(range(1, int(metric_rank.max().max())+1)))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xticks(list(range(0,22,3)))\n",
    "    plt.xlabel('hour')\n",
    "    plt.grid(axis='y')\n",
    "    plt.margins(0.1)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right');\n",
    "    if filename:\n",
    "        if sub_folder:\n",
    "            output_dir = os.path.join(output_dir, sub_folder)\n",
    "        save_plot(plt, filename, output_dir = output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
