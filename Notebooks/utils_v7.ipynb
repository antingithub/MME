{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import os, sys\n",
    "import re\n",
    "import calendar\n",
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.style.use('_classic_test')\n",
    "# plt.style.use('seaborn-paper')\n",
    "# plt.style.use('seaborn-pastel')\n",
    "# plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configurations_v{}.yaml'.format(version), 'r') as out_stream:\n",
    "    CONFIG = yaml.load(out_stream, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['bias', 'sigma', 'perkins', 'yk']),\n",
       " dict_keys(['DJF', 'MAM', 'JJA', 'SON']))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset_path = CONFIG['observations']['temperature']['dataset_path']\n",
    "temp_output_path = CONFIG['observations']['temperature']['output_path']\n",
    "prec_dataset_path = CONFIG['observations']['percepitation']['dataset_path']\n",
    "prec_output_path = CONFIG['observations']['percepitation']['output_path']\n",
    "temp_obs_col = CONFIG['observations']['temperature']['column']\n",
    "\n",
    "cordex_dataset_path = CONFIG['models']['cordex']['dataset_path']\n",
    "cordex_output_path = CONFIG['models']['cordex']['output_path']\n",
    "cordex_start_year = CONFIG['models']['cordex']['time']['start']['year']\n",
    "cordex_hour_interval = CONFIG['models']['cordex']['time']['hour_interval']\n",
    "cordex_start_timestamp = CONFIG['models']['cordex']['time']['timestamp']['start']\n",
    "cordex_end_timestamp = CONFIG['models']['cordex']['time']['timestamp']['end']\n",
    "cordex_freq_timestamp = CONFIG['models']['cordex']['time']['timestamp']['freq']\n",
    "cordex_temp_filename = CONFIG['models']['cordex']['variables']['temperature']['filename']\n",
    "\n",
    "surfex_dataset_path = CONFIG['models']['surfex']['dataset_path']\n",
    "surfex_output_path = CONFIG['models']['surfex']['output_path']\n",
    "surfex_start_year = CONFIG['models']['surfex']['time']['start']['year']\n",
    "surfex_hour_interval = CONFIG['models']['surfex']['time']['hour_interval']\n",
    "surfex_start_timestamp = CONFIG['models']['surfex']['time']['timestamp']['start']\n",
    "surfex_end_timestamp = CONFIG['models']['surfex']['time']['timestamp']['end']\n",
    "surfex_freq_timestamp = CONFIG['models']['surfex']['time']['timestamp']['freq']\n",
    "surfex_temp_filename = CONFIG['models']['surfex']['variables']['temperature']['filename']\n",
    "\n",
    "cordex_dict_raw = CONFIG['models']['cordex']['raw_names']\n",
    "surfex_dict_raw = CONFIG['models']['surfex']['raw_names']\n",
    "models_cols = CONFIG['models']['columns']\n",
    "models_markers = CONFIG['plots']['names_markers']\n",
    "\n",
    "plots_output_path = CONFIG['plots']['output']\n",
    "seasons_dict = CONFIG['dates']['seasons_dict']\n",
    "period = CONFIG['period']['historical']\n",
    "metrics_fn = CONFIG['metrics']['func']\n",
    "metrics_symbols = CONFIG['metrics']['symbols']\n",
    "\n",
    "ens_alpha_col = CONFIG['ensembles']['alpha']['column']\n",
    "ens_beta_col = CONFIG['ensembles']['beta']['column']\n",
    "\n",
    "\n",
    "METRIC_NAMES = metrics_fn.keys()\n",
    "SEASONS = seasons_dict.keys()\n",
    "METRIC_NAMES, SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('temp_dataset_path', '..\\\\data\\\\observations\\\\temp'),\n",
       " ('temp_output_path', '..\\\\data\\\\observations\\\\temp\\\\output'),\n",
       " ('temp_obs_col', 17),\n",
       " ('prec_dataset_path', '..\\\\data\\\\observations\\\\prec'),\n",
       " ('prec_output_path', '..\\\\data\\\\observations\\\\prec\\\\output'),\n",
       " ('cordex_dataset_path', '..\\\\data\\\\cordex'),\n",
       " ('cordex_output_path', '..\\\\data\\\\cordex\\\\output'),\n",
       " ('cordex_start_year', 1971),\n",
       " ('cordex_hour_interval', 3),\n",
       " ('cordex_start_timestamp', '01-01-1971 03'),\n",
       " ('cordex_end_timestamp', '31-12-2005 21'),\n",
       " ('cordex_freq_timestamp', '3H'),\n",
       " ('cordex_temp_filename', 'Forc2m_TA.txt'),\n",
       " ('surfex_dataset_path', '..\\\\data\\\\surfex'),\n",
       " ('surfex_output_path', '..\\\\data\\\\surfex\\\\output'),\n",
       " ('surfex_start_year', 1971),\n",
       " ('surfex_hour_interval', 3),\n",
       " ('surfex_start_timestamp', '01-01-1971 03'),\n",
       " ('surfex_end_timestamp', '31-12-2005 21'),\n",
       " ('surfex_freq_timestamp', '3H'),\n",
       " ('surfex_temp_filename', 'Hourly_T2m.txt'),\n",
       " ('cordex_dict_raw',\n",
       "  {0: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_CNRM-ALADIN63_v2',\n",
       "   1: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_DMI-HIRHAM5_v2',\n",
       "   2: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   3: 'ICHEC-EC-EARTH_historical_r12i1p1_SMHI-RCA4_v1',\n",
       "   4: 'ICHEC-EC-EARTH_historical_r3i1p1_DMI-HIRHAM5_v2',\n",
       "   5: 'IPSL-IPSL-CM5A-MR_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   6: 'MOHC-HadGEM2-ES_historical_r1i1p1_ICTP-RegCM4-6_v1',\n",
       "   7: 'MOHC-HadGEM2-ES_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   8: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_CLMcom-ETH-COSMO-crCLIM-v1-1_v1',\n",
       "   9: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_DMI-HIRHAM5_v1',\n",
       "   10: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_ICTP-RegCM4-6_v1',\n",
       "   11: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_SMHI-RCA4_v1a',\n",
       "   12: 'MPI-M-MPI-ESM-LR_historical_r3i1p1_GERICS-REMO2015_v1',\n",
       "   13: 'NCC-NorESM1-M_historical_r1i1p1_CLMcom-ETH-COSMO-crCLIM-v1-1_v1',\n",
       "   14: 'NCC-NorESM1-M_historical_r1i1p1_DMI-HIRHAM5_v3',\n",
       "   15: 'NCC-NorESM1-M_historical_r1i1p1_GERICS-REMO2015_v1',\n",
       "   16: 'NCC-NorESM1-M_historical_r1i1p1_SMHI-RCA4_v1'}),\n",
       " ('surfex_dict_raw',\n",
       "  {0: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_CNRM-ALADIN63_v2',\n",
       "   1: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_DMI-HIRHAM5_v2',\n",
       "   2: 'CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   3: 'ICHEC-EC-EARTH_historical_r12i1p1_SMHI-RCA4_v1',\n",
       "   4: 'ICHEC-EC-EARTH_historical_r3i1p1_DMI-HIRHAM5_v2',\n",
       "   5: 'IPSL-IPSL-CM5A-MR_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   6: 'MOHC-HadGEM2-ES_historical_r1i1p1_ICTP-RegCM4-6_v1',\n",
       "   7: 'MOHC-HadGEM2-ES_historical_r1i1p1_SMHI-RCA4_v1',\n",
       "   8: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_CLMcom-ETH-COSMO-crCLIM-v1-1_v1',\n",
       "   9: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_DMI-HIRHAM5_v1',\n",
       "   10: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_ICTP-RegCM4-6_v1',\n",
       "   11: 'MPI-M-MPI-ESM-LR_historical_r1i1p1_SMHI-RCA4_v1a',\n",
       "   12: 'MPI-M-MPI-ESM-LR_historical_r3i1p1_GERICS-REMO2015_v1',\n",
       "   13: 'NCC-NorESM1-M_historical_r1i1p1_CLMcom-ETH-COSMO-crCLIM-v1-1_v1',\n",
       "   14: 'NCC-NorESM1-M_historical_r1i1p1_DMI-HIRHAM5_v3',\n",
       "   15: 'NCC-NorESM1-M_historical_r1i1p1_GERICS-REMO2015_v1',\n",
       "   16: 'NCC-NorESM1-M_historical_r1i1p1_SMHI-RCA4_v1'}),\n",
       " ('models_markers',\n",
       "  {0: ('CNRMCM5_ALADIN', '*'),\n",
       "   1: ('CNRMCM5_HIRHAM', '*'),\n",
       "   2: ('CNRMCM5_SMHIRCA', '*'),\n",
       "   3: ('ECEARTH_SMHIRCA', '+'),\n",
       "   4: ('ECEARTH_HIRHAM', '+'),\n",
       "   5: ('IPSLCM5-SMHIRCA', 'd'),\n",
       "   6: ('HADGEM_REGCM', '1'),\n",
       "   7: ('HADGEM_SMHIRCA', '1'),\n",
       "   8: ('MPIESM_COSMO', 'o'),\n",
       "   9: ('MPIESM_HIRHAM', 'o'),\n",
       "   10: ('MPIESM_REGCM', 'o'),\n",
       "   11: ('MPIESM_SMHIRCA', 'o'),\n",
       "   12: ('MPIESM_REMO', 'o'),\n",
       "   13: ('NORESM_COSMO', '^'),\n",
       "   14: ('NORESM_HIRHAM', '^'),\n",
       "   15: ('NORESM_REMO', '^'),\n",
       "   16: ('NORESM_SMHIRCA', '^'),\n",
       "   18: ('EnsembleBeta', '$B$'),\n",
       "   19: ('EnsembleAlfa', '$A$')}),\n",
       " ('plots_output_path', '..\\\\plots\\\\output'),\n",
       " ('seasons_dict',\n",
       "  {'DJF': [12, 1, 2], 'MAM': [3, 4, 5], 'JJA': [6, 7, 8], 'SON': [9, 10, 11]}),\n",
       " ('period', 'historical'),\n",
       " ('metrics_fn',\n",
       "  {'bias': 'bias',\n",
       "   'sigma': 'sigma_score',\n",
       "   'perkins': 'perkins_skill_score',\n",
       "   'yk': 'yk_skewness'}),\n",
       " ('metrics_symbols',\n",
       "  {'bias': '|Bias|',\n",
       "   'perkins': '|S|',\n",
       "   's': '|S|',\n",
       "   'sigma': '|$\\\\sigma_n^{-1}$|',\n",
       "   'yk': '|YK|'}),\n",
       " ('ens_alpha_col', 19),\n",
       " ('ens_beta_col', 18),\n",
       " ('models_cols', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(['temp_dataset_path',\n",
    "'temp_output_path',\n",
    "'temp_obs_col',\n",
    "'prec_dataset_path',\n",
    "'prec_output_path',\n",
    "'cordex_dataset_path',\n",
    "'cordex_output_path',\n",
    "'cordex_start_year',\n",
    "'cordex_hour_interval',\n",
    "'cordex_start_timestamp',\n",
    "'cordex_end_timestamp',\n",
    "'cordex_freq_timestamp',\n",
    "'cordex_temp_filename',\n",
    "'surfex_dataset_path',\n",
    "'surfex_output_path',\n",
    "'surfex_start_year',\n",
    "'surfex_hour_interval',\n",
    "'surfex_start_timestamp',\n",
    "'surfex_end_timestamp',\n",
    "'surfex_freq_timestamp',\n",
    "'surfex_temp_filename',\n",
    "'cordex_dict_raw',\n",
    "'surfex_dict_raw',\n",
    "'models_markers',\n",
    "'plots_output_path',\n",
    "'seasons_dict',\n",
    "'period',\n",
    "'metrics_fn',\n",
    "'metrics_symbols',\n",
    "'ens_alpha_col',\n",
    "'ens_beta_col',\n",
    "'models_cols'],\n",
    "[temp_dataset_path,\n",
    "temp_output_path,\n",
    "temp_obs_col,\n",
    "prec_dataset_path,\n",
    "prec_output_path,\n",
    "cordex_dataset_path,\n",
    "cordex_output_path,\n",
    "cordex_start_year,\n",
    "cordex_hour_interval,\n",
    "cordex_start_timestamp,\n",
    "cordex_end_timestamp,\n",
    "cordex_freq_timestamp,\n",
    "cordex_temp_filename,\n",
    "surfex_dataset_path,\n",
    "surfex_output_path,\n",
    "surfex_start_year,\n",
    "surfex_hour_interval,\n",
    "surfex_start_timestamp,\n",
    "surfex_end_timestamp,\n",
    "surfex_freq_timestamp,\n",
    "surfex_temp_filename,\n",
    "cordex_dict_raw,\n",
    "surfex_dict_raw,\n",
    "models_markers,\n",
    "plots_output_path,\n",
    "seasons_dict,\n",
    "period,\n",
    "metrics_fn,\n",
    "metrics_symbols,\n",
    "ens_alpha_col,\n",
    "ens_beta_col,\n",
    "models_cols\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cordex/surfex models dict to YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cordex_dict_raw = {i:name for i,name in enumerate(select_models_folders(cordex_dataset_path)[0])}\n",
    "# surfex_dict_raw = {i:name for i,name in enumerate(select_models_folders(surfex_dataset_path)[0])}\n",
    "\n",
    "# models_markers = {\n",
    "#     0: ('CNRMCM5_ALADIN', '*'),\n",
    "#     1: ('CNRMCM5_HIRHAM', '*'),\n",
    "#     2: ('CNRMCM5_SMHIRCA', '*'),\n",
    "#     3: ('ECEARTH_SMHIRCA', '+'),\n",
    "#     4: ('ECEARTH_HIRHAM', '+'),\n",
    "#     5: ('IPSLCM5-SMHIRCA', 'd'),\n",
    "#     6: ('HADGEM_REGCM', '1'),\n",
    "#     7: ('HADGEM_SMHIRCA', '1'),\n",
    "#     8: ('MPIESM_COSMO', 'o'),\n",
    "#     9: ('MPIESM_HIRHAM', 'o'),\n",
    "#     10: ('MPIESM_REGCM', 'o'),\n",
    "#     11: ('MPIESM_SMHIRCA', 'o'),\n",
    "#     12: ('MPIESM_REMO', 'o'),\n",
    "#     13: ('NORESM_COSMO', '^'),\n",
    "#     14: ('NORESM_HIRHAM', '^'),\n",
    "#     15: ('NORESM_REMO', '^'),\n",
    "#     16: ('NORESM_SMHIRCA', '^')}\n",
    "\n",
    "# metrics_symbols = {'bias': '|Bias|', 'sigma': r'|$\\sigma_n^{-1}$|',\n",
    "#                 'perkins': '|S|', 's': '|S|', 'yk': '|YK|'}\n",
    "\n",
    "# with open('configurations_v7.yaml', 'a') as in_stream:\n",
    "#     yaml.dump(metrics_symbols, in_stream)\n",
    "# #     yaml.dump(models_markers, in_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE!\n",
    "# REDO\n",
    "\n",
    "\n",
    "def get_xls_files(variable, obs_path):\n",
    "    '''\n",
    "        variable: 'temp' or 'prec'\n",
    "        glob_path: path to file type excel, used by glob\n",
    "            example: '..\\\\data\\\\observations\\\\*.xls'\n",
    "            \n",
    "        Returns a dict of all file_paths in the following format\n",
    "            file_path': (variable, year)\n",
    "            item example: '..\\\\data\\\\observations\\\\PREC1979.XLS': ('prec', '1979')\n",
    "\n",
    "    '''\n",
    "    glob_path = os.path.join(os.getcwd(), obs_path) + '\\*.xls'    # the '\\'  in '\\*.xls' is nedded\n",
    "    print(glob_path)\n",
    "    \n",
    "    def all_4digits_year(dict_files):\n",
    "        '''\n",
    "            returns the dict of files with porper year yyyy\n",
    "        '''\n",
    "        return {file:(pair[0], '19'+ pair[1] if len(pair[1]) == 2 else pair[1]) for file,pair in dict_files.items()}\n",
    "\n",
    "    file_pattern = re.compile(rf'{variable}\\s*(?P<year>\\d*).xls' , flags=re.IGNORECASE)\n",
    "    files = {f:(variable, re.search(file_pattern, os.path.basename(f)).group(1)) for f in glob(glob_path) if re.match(file_pattern, os.path.basename(f))}\n",
    "    return all_4digits_year(files)\n",
    "\n",
    "\n",
    "def get_data_from_xls(filename_path, year):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    xls = pd.ExcelFile(filename_path)\n",
    "    df_raw = pd.read_excel(xls, header=None, usecols=range(0,25), index=False)\n",
    "    # clean rows with no data (first col empty, must have a day or a month)\n",
    "    df_raw.dropna(axis=0, how='all', subset=[0], inplace=True)\n",
    "\n",
    "    # df to hold final result\n",
    "    columns = ['month', 'day', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    df_temp = pd.DataFrame(columns = columns)\n",
    "\n",
    "    months_list = ['jan', 'fev', 'mar', 'abr', 'mai', 'jun', 'jul', 'ago', 'set', 'out', 'nov', 'dez', 'stop']\n",
    "    months_dict = {m:i for i,m in enumerate(months_list[:-1], 1)}\n",
    "    def month_iter(months):     # allows to consume all the months once, it uses a 'stop' word to easing the parser\n",
    "        for month in months:\n",
    "            yield month\n",
    "\n",
    "    # regex for 1-31\n",
    "    day_pattern = re.compile(r'(?P<day>([1-9]|[12]\\d|3[01]))$') \n",
    "\n",
    "    # month iterator (can be done with a simple list as well)\n",
    "    month_it = month_iter(months_list)\n",
    "    month = next(month_it) # starts with 'jan'\n",
    "\n",
    "    for row in df_raw.iterrows():\n",
    "        label = row[1][0]                                                  # for each row the first col\n",
    "        month_pattern = re.compile(rf'{month}' , flags=re.IGNORECASE)\n",
    "        if re.match(month_pattern, str(label)):                      # assumes that the first line has the 'jan' matching pattern\n",
    "            prev_month = month\n",
    "            try:\n",
    "                month = next(month_it)\n",
    "            except:\n",
    "                break                                               # the end of the list\n",
    "        if re.match(day_pattern, str(label)):\n",
    "            day = label                                             # it is a day\n",
    "            new_series = pd.Series([months_dict[prev_month], day]).append(row[1][1:]) # select only the values (temp/prec)\n",
    "            new_series.index = df_temp.columns\n",
    "            df_temp = df_temp.append(new_series, ignore_index=True)\n",
    "    df_temp['year'] = year      \n",
    "    return df_temp[['year'] + columns]\n",
    "\n",
    "\n",
    "\n",
    "def get_all_obs_data(dict_files, variable, out_dir = None):\n",
    "    '''\n",
    "        Returns a df with all temperature series\n",
    "        Numbers are rounded to 2 decimal places\n",
    "        Saves the df to  ..\\\\data\\\\observations\\\\output\\\\\n",
    "    '''\n",
    "    # order file paths by year, ascending.\n",
    "    files_years = sorted([(path,year[1]) for path,year in dict_files.items()], key = lambda v: v[1])\n",
    "\n",
    "    columns = ['year', 'month', 'day', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    df_data = pd.DataFrame(columns = columns)\n",
    "\n",
    "    for file_path,year in files_years:\n",
    "        df_temp = get_data_from_xls(file_path, year)\n",
    "        assert (df_temp.shape[0] <= 366) and (df_temp.shape[0] >= 365), '365 or 366 days in file'\n",
    "        print('file_path - {}\\tyear - {}\\tlen: {}'.format(file_path, year, df_temp.shape[0]))\n",
    "        df_data = df_data.append(df_temp)\n",
    "\n",
    "    # fix dtypes    \n",
    "    dict_types = {col:'float' for col in df_data.columns}\n",
    "    dict_types['year'], dict_types['month'], dict_types['day']  = 'int', 'int', 'int'\n",
    "    df_data = df_data.astype(dict_types)\n",
    "    # round to 2 decimal places\n",
    "    df_data = df_data.round(decimals = {col:2 for col in columns[3:]})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # save dataframe to csv\n",
    "    filename = '{}_{}_{}.csv'.format(variable, files_years[0][1], files_years[-1][1])\n",
    "    save_df2csv(df_data, filename, out_dir)\n",
    "    return df_data.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_models_folders(dir_models = None, period = period):\n",
    "    folders = [folder for folder in  os.listdir(os.path.join(os.getcwd(), dir_models)) if period in folder]\n",
    "    return folders, len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season2df(df):\n",
    "    cols = df.columns\n",
    "    conditions = [(df['month'].isin([12, 1, 2])), (df['month'].isin([3, 4, 5])),\n",
    "                (df['month'].isin([6, 7, 8])), (df['month'].isin([9, 10, 11]))]\n",
    "    df['season'] = np.select(conditions, seasons_dict.keys())\n",
    "    cols = cols.insert(1, 'season')\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_models(dir_models = None, \n",
    "                    var_filename = None, \n",
    "                    start_timestamp = cordex_start_timestamp,\n",
    "                    end_timestamp = cordex_end_timestamp,\n",
    "                    freq_timestamp = cordex_freq_timestamp, plot = False):\n",
    "    '''\n",
    "        For temperature only!! Convertion to C\n",
    "        \n",
    "    '''\n",
    "    res = []                        # to accumulate the dataseries\n",
    "    color = cycle('rbgkmc')\n",
    "    folders, nmodels = select_models_folders(dir_models, period)\n",
    "    for model_id, folder in enumerate(folders):\n",
    "        T2m = np.loadtxt(os.path.join(dir_models, folder, var_filename)) - 273.15\n",
    "        T2m = pd.Series(T2m, name = model_id, index=pd.date_range(\n",
    "                                                    start = start_timestamp,\n",
    "                                                    end = end_timestamp,\n",
    "                                                    freq = freq_timestamp,\n",
    "                                                    dayfirst=True ))\n",
    "        # replace -9999 for np.nan\n",
    "        if T2m[0] < -100: T2m[0] = np.nan\n",
    "            \n",
    "        if plot:\n",
    "            T2m.plot(figsize=(18,8), color=next(color))\n",
    "            plt.title('Model: {}'.format(model_id))\n",
    "            plt.show();\n",
    "        res.append(T2m) \n",
    "        \n",
    "    res = pd.concat(res, axis = 1)\n",
    "    res.index.name = 'date'\n",
    "    \n",
    "    if plot:\n",
    "        res.plot(figsize=(18,8))\n",
    "        plt.tight_layout()\n",
    "        plt.show();\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datafile(save_fn, filename, output_dir):\n",
    "    '''\n",
    "        out_filename: defines what is computed, ex. temp_sea_avg_obs\n",
    "    '''\n",
    "    out_dir = os.path.join(os.getcwd(), output_dir)\n",
    "    if not os.path.exists(out_dir):\n",
    "        print('Output folder does not exist. Created a new one: {}'.format(out_dir))\n",
    "        os.makedirs(out_dir)\n",
    "    if filename[-4:] != '.csv':\n",
    "        filename = filename + '.csv'\n",
    "    save_fn(path_or_buf = os.path.join(os.getcwd(), out_dir, filename))\n",
    "\n",
    "\n",
    "def save_df2csv(df, filename, output_dir, index=False):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    # to get other args when called\n",
    "    save_fn = partial(df.to_csv, sep=',', header=True, index=index, date_format='%Y-%m-%d %H:%M:%S')\n",
    "    return save_datafile(save_fn, filename, output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_plot(plot, filename, output_dir = plots_output_path):\n",
    "    '''\n",
    "        out_filename: defines what is computed, ex. temp_sea_avg_obs\n",
    "    '''\n",
    "    out_dir = os.path.join(os.getcwd(), output_dir)\n",
    "    if not os.path.exists(out_dir):\n",
    "        print('Output folder does not exist. Created a new one: {}'.format(out_dir))\n",
    "        os.makedirs(out_dir)\n",
    "    path_or_buf = os.path.join(os.getcwd(), out_dir, filename)\n",
    "    plot.savefig(path_or_buf,\n",
    "                 dpi=None,\n",
    "                 facecolor='w',\n",
    "                 edgecolor='w',\n",
    "                 orientation='portrait',\n",
    "                 papertype=None, format=None,\n",
    "                 transparent=False,\n",
    "                 bbox_inches=None,\n",
    "                 pad_inches=0.1,\n",
    "                 frameon=None,\n",
    "                 metadata=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv2df(filename, filepath, freq_index=None):\n",
    "    '''\n",
    "        if it is a timeseries 'date' and a 'freq_index' value is passed it is the index\n",
    "    '''\n",
    "    def conv_to_int(c):    # to guarantee that the hours' column names are type int\n",
    "        try:\n",
    "            c = int(c)\n",
    "        except:\n",
    "            return c\n",
    "        return c\n",
    "    \n",
    "    filename_path = os.path.join(os.getcwd(), filepath, filename)\n",
    "    date_dtype = {'year':np.int32, 'month':np.int32, 'day':np.int32}\n",
    "    df = pd.read_csv(filename_path, sep=',', parse_dates=True, dtype = date_dtype)\n",
    "    \n",
    "    df.columns = [conv_to_int(col) for col in df.columns]\n",
    "    if ('date' in df.columns) and (freq_index):\n",
    "        df.set_index('date', inplace=True) # if it is a timeseries 'date' is the index\n",
    "        df.index = pd.DatetimeIndex(df.index.values, freq=freq_index)\n",
    "        df.index.name = 'date'\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def compare_saved_loaded_df(df_, df):\n",
    "    assert df_.index.equals(df.index)\n",
    "    assert df_.columns.equals(df.columns)\n",
    "    assert np.isclose(df_.values, df.values, equal_nan=True).all()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_ts_format(df, existing_cols, new_col_name, var_name, freq_index='3H'):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df_temp = df.copy(deep=True)\n",
    "    df_temp = pd.melt(df_temp, id_vars=existing_cols, var_name=new_col_name, value_name=var_name)\n",
    "\n",
    "    df_temp = add_season2df(df_temp)                  # add season column\n",
    "    df_temp['hour'] = df_temp['hour'].astype(int)     # 'hour' must be int\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_temp['date'] = df_temp.apply(lambda s: pd.datetime(*map(int, (s.year, s.month, s.day, s.hour)))\n",
    "                                  , axis=1)\n",
    "    df_temp = df_temp.set_index('date')\n",
    "    df_temp.drop(['year','month','day','hour'], axis=1, inplace=True)\n",
    "    df_temp.sort_index(inplace=True)\n",
    "    df_temp = df_temp.asfreq(freq_index)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_season(df, season = None, month_col='month'):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    return df.loc[df['month'].isin(seasons_dict[season]), :]\n",
    "\n",
    "\n",
    "def filter_ts_by_hour(df, hour = None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    return df[df.index.hour==hour]\n",
    "\n",
    "\n",
    "\n",
    "def filter_ts_by_season(ts, season = None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    ts = ts.copy()\n",
    "    return ts.loc[ts.index.month.isin(seasons_dict[season]), :]\n",
    "\n",
    "\n",
    "def df_all_models(list_df_models):\n",
    "    cols = list_df_models[0].columns\n",
    "    for cmod,model in enumerate(list_df_models):\n",
    "        model['model'] = cmod\n",
    "    df =  pd.concat([*list_df_models], axis=0)\n",
    "    return df[['model', *cols]]\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(ts):\n",
    "    ts = ts.reset_index()\n",
    "    ts.drop_duplicates(inplace=True)\n",
    "    ts.set_index('date', inplace=True)\n",
    "    ts = ts.iloc[:,0]\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentiles_table(df, values, by = 'hour', percentiles = [1,5,25,50,75,95,99]):\n",
    "    '''\n",
    "        Returns the percentiles of a value by a specific column (by)\n",
    "        Organizes a timeseries in tabular form for a specific column\n",
    "        df header: year\tmonth\tday\thour\ttemp_3h\n",
    "        \n",
    "        values: example 'temp_3h'\n",
    "        \n",
    "    '''\n",
    "    index = [col for col in df.columns if col not in [values, by]]\n",
    "    df  = df.pivot_table(values = values, columns = by, index = index)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df_desc = df.describe(percentiles=np.asarray(percentiles)/100)\n",
    "    df_desc = df_desc.T\n",
    "    df_desc.reset_index(inplace=True)\n",
    "    df_desc.rename(columns={'index': by}, inplace=True)\n",
    "    df_desc.drop(['count', 'std'], axis=1, inplace=True)\n",
    "    df_desc\n",
    "    return df_desc.set_index(by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_models_obs(func, df_data, model_cols, df_obs):\n",
    "    '''\n",
    "        Generic apply function that performs a func between a list of columns (model_cols) and a single column (df_obs)\n",
    "        \n",
    "    '''\n",
    "    return df_data[model_cols].apply(func, obs = df_obs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_(s, obs):\n",
    "    # NO NEED to remove duplicates from df_obs - used for Ensemble Beta\n",
    "    return (obs - s).mean()\n",
    "\n",
    "\n",
    "def bias(df, model_cols, df_obs):\n",
    "    return fn_models_obs(diff_, df, model_cols, df_obs), df.mean(axis=0)      # simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigma score - Normalized standard deviation measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_(s, obs):\n",
    "    # accordingly to the sigma definition\n",
    "    return s.std()/obs.std()\n",
    "\n",
    "\n",
    "def sigma_score(df, model_cols, df_obs):\n",
    "    ### Remove duplicates from df_obs -  used for Ensemble Beta\n",
    "    df_obs = remove_duplicates(df_obs)\n",
    "    return fn_models_obs(sigma_, df, model_cols, df_obs), df.std()    # simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### S score - Perkins Skill score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(series, bins = None):\n",
    "    '''\n",
    "        pdf for pandas dataSeries, using a numpy array func\n",
    "        returns both the density and the bin edges\n",
    "    '''\n",
    "    density, _ = np.histogram(series, bins = bins, density=True)\n",
    "    return density\n",
    "\n",
    "\n",
    "def  minimun(s, obs):\n",
    "    return np.minimum(s, obs)\n",
    "\n",
    "def perkins_skill_score(df, model_cols, df_obs, bins=None):\n",
    "    cols = model_cols   \n",
    "    if not bins:\n",
    "        bins = range(0, 51)   # 0ºC to 50ºC\n",
    "    # only models, see below for observations (needed for EnsembleBeta, observations remove duplicates)\n",
    "    models_pdfs = [pdf(s, bins) for _,s in df[cols].items()]\n",
    "    \n",
    "    ### Remove duplicates from df_obs  -  used for Ensemble Beta\n",
    "    df_obs = remove_duplicates(df_obs)\n",
    "    obs_pdf = pdf(df_obs.values, bins)\n",
    "    # to export as probe together with models pdfs\n",
    "    df_obs_pdf = pd.DataFrame(obs_pdf, columns=[temp_obs_col], index=bins[:-1])\n",
    "    \n",
    "    # transpose it!    \n",
    "    df_pdfs = pd.DataFrame(models_pdfs, index=cols, columns=bins[:-1]).T\n",
    "    # S index calculation\n",
    "    S = fn_models_obs(minimun, df_pdfs, model_cols, obs_pdf).sum(axis=0)*100\n",
    "    return S, pd.concat([df_pdfs, df_obs_pdf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YK skewness - Yule-Kendall skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YK_old(df, model_cols, df_obs, obs_col=17):\n",
    "    models_perc = df[model_cols].describe(percentiles=[0.05, 0.5, 0.95])\n",
    "    ### Remove duplicates from df_obs  -  used for Ensemble Beta\n",
    "    df_obs = remove_duplicates(df_obs)\n",
    "    perc_obs = df_obs.describe(percentiles=[0.05, 0.5, 0.95])\n",
    "    perc_obs.name= obs_col\n",
    "    df_perc = pd.concat([models_perc, perc_obs], axis=1)\n",
    "    df_perc = df_perc.iloc[4:7, :]\n",
    "    return ((df_perc.loc['95%', :] - df_perc.loc['50%', :]) -\\\n",
    "            (df_perc.loc['50%', :] - df_perc.loc['5%', :]))\\\n",
    "            / (df_perc.loc['95%', :] - df_perc.loc['5%', :]), df_perc\n",
    "\n",
    "\n",
    "\n",
    "def yk_perc(df):\n",
    "    return ((df.loc['95%', :] - df.loc['50%', :]) -\\\n",
    "            (df.loc['50%', :] - df.loc['5%', :]))\\\n",
    "            / (df.loc['95%', :] - df.loc['5%', :])\n",
    "\n",
    "def YK_(df, model_cols, df_obs, obs_col=17):\n",
    "    models_perc = df[model_cols].describe(percentiles=[0.05, 0.5, 0.95])\n",
    "    ### Remove duplicates from df_obs  -  used for Ensemble Beta\n",
    "    df_obs = remove_duplicates(df_obs)\n",
    "    perc_obs = df_obs.describe(percentiles=[0.05, 0.5, 0.95])\n",
    "    perc_obs.name= obs_col\n",
    "    df_perc = pd.concat([models_perc, perc_obs], axis=1)\n",
    "    df_perc = df_perc.iloc[4:7, :]\n",
    "    return yk_perc(df_perc), df_perc\n",
    "\n",
    "\n",
    "def yk_diff(s, obs):\n",
    "    return s-obs\n",
    "\n",
    "def YK_skewness_by_hour(df, model_cols, obs_col):\n",
    "    '''\n",
    "        model_cols: list of columns\n",
    "    '''\n",
    "    df_yk, probe = metric_by_hour(YK_, df, model_cols, obs_col)\n",
    "    return df_yk.loc[model_cols, :].apply(yk_diff, obs = df_yk.loc[obs_col, :], axis=1), probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_by_hour(metric, df, lst_model_cols, observation_col):\n",
    "    '''\n",
    "        with a time series as input, first is filtered for each hour (daily cycle)\n",
    "        calls metric() to get the metric results, and gathers to a list of results\n",
    "        \n",
    "        Returns:\n",
    "            df with all results organized\n",
    "                    values: metric\n",
    "                    cols: hours\n",
    "                    rows: models\n",
    "            probes\n",
    "    '''\n",
    "    res = []\n",
    "    probes = []   # to gather intermediate results. Ex: In perkins the models pdfs\n",
    "    for hour in range(0,22,3):\n",
    "        df_filtered = filter_ts_by_hour(df, hour)\n",
    "#         print(df_filtered.shape, df.shape)\n",
    "        df_obs = df_filtered.loc[:,observation_col]    # separate the observations df\n",
    "        df_metric, probe = metric(df_filtered, lst_model_cols, df_obs)\n",
    "#         df_metric, probe = metric(df_filtered.iloc[:,:-1], lst_model_cols, df_obs)\n",
    "        res.append(df_metric)        \n",
    "        probes.append(probe)\n",
    "    return pd.concat(res, axis=1, keys=list(range(0,22,3))), probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pdf, cdf, pmf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdf_cdf(sample, plot=True):\n",
    "    '''\n",
    "        Visualize the ECDF, interpolated PDF and samples\n",
    "        \n",
    "    '''\n",
    "    bins=int(np.sqrt(len(sample)))\n",
    "#     bins =  100\n",
    "    hist = np.histogram(sample, bins = bins)\n",
    "    hist_dist = st.rv_histogram(hist)\n",
    "\n",
    "    sample_min, sample_max = np.min(sample), np.max(sample)\n",
    "    pdf, Ecdf = hist_dist.pdf, hist_dist.cdf\n",
    "    assert Ecdf(sample_min) == 0, 'Something wrong with ECDF!'\n",
    "    assert Ecdf(sample_max) == 1, 'Something wrong with ECDF!'\n",
    "\n",
    "    if plot:\n",
    "        X = np.linspace(sample_min, sample_max, bins)\n",
    "        plt.title(\"PDF\")\n",
    "        _ = plt.hist(sample, density = True, bins = bins)\n",
    "        _ = plt.plot(X, pdf(X), label = 'PDF')\n",
    "        #_ = plt.plot(X, Ecdf(X), label = 'CDF')\n",
    "        plt.legend()\n",
    "        plt.margins(0.02)\n",
    "        plt.show()\n",
    "    return pdf, Ecdf\n",
    "\n",
    "\n",
    "def compute_pdf_KDE(sample, bandwidth=2, kernel='gaussian', plot=True):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    bins=int(np.sqrt(len(sample)))\n",
    "    #print(bins)\n",
    "    sample = sample.reshape((len(sample), 1))\n",
    "    # fit density\n",
    "    model = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n",
    "    model.fit(sample)\n",
    "    \n",
    "    if plot:\n",
    "        sample_min, sample_max = np.min(sample), np.max(sample)\n",
    "        X = np.linspace(sample_min, sample_max, bins)\n",
    "        values = X\n",
    "        values = values.reshape((len(values), 1))\n",
    "        #print(values.shape)\n",
    "        probabilities = model.score_samples(values)\n",
    "        probabilities = np.exp(probabilities)\n",
    "        _ = plt.hist(sample, bins=bins, density=True)\n",
    "        _ = plt.plot(values[:], probabilities)\n",
    "        plt.title(\"pdf using KDE\")\n",
    "        plt.show()\n",
    "    #return np.exp(model.score_samples)\n",
    "    return model.score_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metrics_by_hour(df, title,\n",
    "                        filename = None,\n",
    "                        output_dir = plots_output_path,\n",
    "                        sub_folder=None,\n",
    "                        df_other=None):\n",
    "    '''\n",
    "        Sigma is not Sigma^-1\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 30), sharex=True, sharey=True)\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 10), sharex=True, sharey=True)  # figsize=(w, h)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    \n",
    "    # plot all models and  FIX IT!!!!!!\n",
    "    n_models2plot = df.index[-1]+1\n",
    "\n",
    "    \n",
    "    for hour,ax in zip(range(0,22,3), axs.flat):\n",
    "        plt.subplot(ax)\n",
    "        ax.plot(df[hour], '-o')\n",
    "        if isinstance(df_other, pd.DataFrame):\n",
    "            plt.plot(df_other[hour], '-o')\n",
    "        ax.set_title('Hour = {}'.format(hour), fontsize=14)\n",
    "        plt.grid(axis='x', color='gray', linestyle='--')\n",
    "        plt.xticks(list(range(n_models2plot)))\n",
    "        ax.margins(0.05)\n",
    "    if isinstance(df_other, pd.DataFrame):\n",
    "        plt.legend(labels=['bias corrected'], loc='best')\n",
    "    if filename:\n",
    "        if sub_folder:\n",
    "            output_dir = os.path.join(output_dir, sub_folder)\n",
    "        save_plot(plt, filename+'_per_h', output_dir = output_dir)\n",
    "    plt.show();\n",
    "    \n",
    "    df.plot(figsize=(15, 10), style = '-o')\n",
    "    fig.legend(labels = df.columns, loc='upper right', title='hour')\n",
    "    plt.xticks(list(range(n_models2plot)))\n",
    "    plt.margins(0.05)\n",
    "    plt.grid(axis='x', color='silver', linestyle='--')\n",
    "    plt.title(title, fontsize=20)\n",
    "    if filename:\n",
    "        save_plot(plt, filename, output_dir = output_dir)\n",
    "    print(output_dir)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ranks(df_metric, metric=None, scale=None, decimals=None): \n",
    "    if metric == 'bias':\n",
    "        scale = 7\n",
    "        metric_prep = df_metric.abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    elif metric == 'perkins':\n",
    "        scale = 100\n",
    "        metric_prep = df_metric.div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=True)\n",
    "    \n",
    "    elif metric == 'sigma':\n",
    "        scale = 2\n",
    "        metric_prep = (df_metric - 1).abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    elif metric == 'yk':\n",
    "        scale = 1\n",
    "        metric_prep = df_metric.abs().div(scale).round(decimals=1)\n",
    "        metric_rank = metric_prep.rank(method='dense', ascending=False)\n",
    "    \n",
    "    else:\n",
    "        print('{} is not a valid metric [bias, sigma, perkins, yk]'.format(metric))\n",
    "    \n",
    "    print('abs({})\\tmin:{}\\tmax:{} '.format(metric, df_metric.abs().min().min(), df_metric.abs().max().max()))\n",
    "    print('transf({})\\tmin:{}\\tmax:{} '.format(metric, metric_prep.abs().min().min(), metric_prep.abs().max().max()))\n",
    "    print('{} rank\\tmin:{}\\tmax:{} '.format(metric, metric_rank.abs().min().min(), metric_rank.abs().max().max()))\n",
    "    \n",
    "    return metric_rank\n",
    "\n",
    "\n",
    "def plot_bump(metric_rank,\n",
    "              title,\n",
    "              filename=None,\n",
    "              output_dir = plots_output_path,\n",
    "              sub_folder=None, \n",
    "              figsize=(18,18)):\n",
    "    '''\n",
    "        SIGMA?????\n",
    "    \n",
    "    '''\n",
    "    for row in metric_rank.iterrows():\n",
    "        delta = row[0]*0.025\n",
    "        metric_rank.iloc[row[0],:] = row[1] + delta\n",
    "\n",
    "    metric_rank.T.plot(figsize=(18,18), style='-o')\n",
    "    plt.yticks(list(range(1, int(metric_rank.max().max())+1)))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xticks(list(range(0,22,3)))\n",
    "    plt.xlabel('hour')\n",
    "    plt.grid(axis='y')\n",
    "    plt.margins(0.1)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right');\n",
    "    if filename:\n",
    "        if sub_folder:\n",
    "            output_dir = os.path.join(output_dir, sub_folder)\n",
    "        save_plot(plt, filename, output_dir = output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models' comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Urb_effect(data):\n",
    "    '''\n",
    "        artificialy create data with urban effect\n",
    "    \n",
    "    '''\n",
    "    data = data.copy()\n",
    "    mu = df_bias_jja.values.reshape(1,-1).mean()\n",
    "    std = df_bias_jja.values.reshape(1,-1).std()\n",
    "    return data + std*np.random.randn(*df_bias_jja.shape) + mu\n",
    "\n",
    "\n",
    "\n",
    "def aux_plot_cmp_metric(metric, ax, hour, data, d_min, d_max, id_letter):#, param_dict):\n",
    "    \"\"\"\n",
    "    A helper function to make a metric comparison graph\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : Axes. The axes to draw to\n",
    "    data : Dataframe. The data as a df with 2 cols\n",
    "    d_min: the floor.min of all data points\n",
    "    d_max: the ceil.max of all data points\n",
    "    id_letter: letters to identify the subplots\n",
    "    #param_dict : dict. Dictionary of kwargs to pass to ax.plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        list of artists added\n",
    "    \"\"\"\n",
    "    metric = metric.lower()\n",
    "#     metrics_dict = {'bias': '|Bias|', 'sigma': r'|$\\sigma_n^{-1}$|',    # TODO - mov to configurations file\n",
    "#                     'perkins': '|S|', 's': '|S|', 'yk': '|YK|'}\n",
    "    for idx,row in data.iterrows():\n",
    "        out = ax.scatter(row[0], row[1], \n",
    "                         label=models_markers[idx][0], \n",
    "                         marker=models_markers[idx][1],\n",
    "                         s=100,)\n",
    "                        # **param_dict)\n",
    "    if metric in ['bias', 'sigma', 'yk']:\n",
    "        ax.fill([d_min, d_min, d_max, d_min], [d_min, d_max, d_max, d_min], fill=True, alpha=0.3, color='lightgray')\n",
    "    elif metric in ['perkins', 's']: # only for perkins\n",
    "        ax.fill([d_min, d_max, d_max, d_min], [d_min, d_min, d_max, d_min], fill=True, alpha=0.3, color='lightgray')\n",
    "    else:\n",
    "        print('Metric not allowed! Chose among: bias, perkins, sigma, yk')\n",
    "    \n",
    "    ax.set_xlabel(' '.join(('CORDEX {}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "    ax.set_ylabel(' '.join(('SFX {}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "    \n",
    "    # outer limits\n",
    "    dd_min, dd_max= d_min-0.05*d_max, d_max+0.05*d_max\n",
    "    ax.set_xlim(dd_min, dd_max)\n",
    "    ax.set_ylim(dd_min, dd_max)\n",
    "    ax.text(-0.15, 0.95, '{})'.format(next(id_letter)),\n",
    "            fontsize= 16, transform=ax.transAxes)\n",
    "    ax.text(0.5, 0.95, r'$hour={}h$'.format(hour),\n",
    "        horizontalalignment='center', fontsize=14, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_cmp_metric(metric, data_ref, data_cmp, title_fig=None,\n",
    "                    out_filename=None, out_dir=plots_output_path, sub_folder=None ):\n",
    "    '''\n",
    "        OBS:\n",
    "            Computed the abs for all metrics: |Bias|,|Perkins|,|YK| and\n",
    "            |Sigma-1|\n",
    "    '''\n",
    "    # fig dimensions\n",
    "    W, H = 12, 24\n",
    "    \n",
    "    \n",
    "    if metric == 'sigma':\n",
    "        data_ref = data_ref - 1\n",
    "        data_cmp = data_cmp - 1\n",
    "    # ABS\n",
    "    data_ref, data_cmp = data_ref.abs(), data_cmp.abs()\n",
    "    \n",
    "    data_x_min, data_x_max = data_ref.min().min(), data_ref.max().max()\n",
    "    data_y_min, data_y_max = data_cmp.min().min(), data_cmp.max().max()\n",
    "    x_min = min([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "    x_max = max([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "    #x_min = np.floor(x_min)\n",
    "    if (x_max > 1) and (metric!='sigma'): x_max = np.ceil(x_max)                            # TODO BETTER\n",
    "\n",
    "    from itertools import cycle\n",
    "    plot_id = cycle('abcdefghijklmnopqrstuvxwz')  \n",
    "\n",
    "\n",
    "    hours = range(0,22,3)\n",
    "    fig, axs = plt.subplots(4,2, figsize=(W, H), frameon=True, constrained_layout=False)\n",
    "    axs = axs.flat\n",
    "    for hour, ax in zip(hours, axs):\n",
    "        data = pd.DataFrame()\n",
    "        data['CDX'] = data_ref.loc[:,hour]\n",
    "        data['SFX'] = data_cmp.loc[:,hour]\n",
    "        plot = aux_plot_cmp_metric(metric, ax,\n",
    "                               hour,\n",
    "                               data,\n",
    "                               x_min, x_max,\n",
    "                               plot_id,)\n",
    "                               #param_dict={'zorder':1})\n",
    "\n",
    "    # get legend form last axe\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    leg = fig.legend(handles, labels, \n",
    "                     ncol=3, mode='expand', \n",
    "                     frameon=False, loc=8,\n",
    "                     fontsize=14, markerscale=1)\n",
    "    fig.suptitle('Season: {}'.format(title_fig), fontsize=14)\n",
    "#     fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.1)                         # TRICK\n",
    "    \n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# Generator for time series comparison\n",
    "# hourly means\n",
    "def gen_cdx_sfx_ts_cmp_h_mean(models_seq):\n",
    "    '''\n",
    "        This is a generator - yields\n",
    "        (mean_hourly dataframe , cordex/surfex, season\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    for mod_sea,path in models_seq:\n",
    "        filename = '{}_join_obs_{}.csv'.format(mod_sea[0], mod_sea[1].lower())\n",
    "        df = load_csv2df(filename, path, freq_index='3H')\n",
    "        # calculate the hourly means\n",
    "        mean_hourly = df.groupby(df.index.time).mean()\n",
    "        mean_hourly.index =  list(map(lambda t: t.hour, mean_hourly.index))\n",
    "        mean_hourly.index.name = 'hour'\n",
    "        yield mean_hourly,mod_sea[0],mod_sea[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "def aux_plot_models_obs_hourly_mean(data, ax, title=u'title'):\n",
    "    for col in (data.loc[:,list(range(17))]):\n",
    "        ax.plot(data.loc[:, col], alpha=0.2, label=models_markers[col][0])  # linestyle='--'\n",
    "    ax.plot(data.mean(axis=1), '-o', label='avg')\n",
    "    out = ax.plot(data.loc[:, 17], '-o', color='red', label='obs')\n",
    "    ax.set_xticks(list(range(0,22,3)))\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(r'T$_{2m}$ $^{o}$C', fontsize=14)\n",
    "    ax.set_xlabel('hours')\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_models_obs_hourly_mean(data=None, model_name=None, season=None,\n",
    "                                out_filename=None,\n",
    "                                out_dir=plots_output_path, sub_folder=None):\n",
    "    '''\n",
    "        Plots the models and observations average by hour\n",
    "    \n",
    "    '''\n",
    "    W, H = 18, 8    # fig dimensions\n",
    "    mean_hourly = data.groupby(data.index.time).mean()\n",
    "    mean_hourly.index =  list(map(lambda t: t.hour, mean_hourly.index))\n",
    "    mean_hourly.index.name = 'hour'\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(W, H), frameon=True, constrained_layout=False)\n",
    "    title = \"{}: Average temperature for 3h time interval. All 16 {} models and its average.\".format(*map(str.upper,[season, model_name])) \n",
    "    out = aux_plot_models_obs_hourly_mean(mean_hourly, ax, title=title)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    leg = ax.legend(handles[-2:], labels[-2:], frameon=False, loc=4, fontsize=14)\n",
    "    \n",
    "    \n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def plot_cmp_models_obs_hourly_mean(out_filename=None, title=u'title', out_dir=None, sub_folder=None):\n",
    "\n",
    "    W, H = 18,28 # fig dimensions\n",
    "    # composing (('cordex', 'DJF'), '..\\\\data\\\\cordex\\\\output'),  (('surfex', 'DJF'), '..\\\\data\\\\surfex\\\\output'),...,(('surfex', 'SON'), '..\\\\data\\\\surfex\\\\output')\n",
    "    gen = zip(((mod,sea) for sea in seasons_dict.keys() for mod in ['cordex','surfex']),\n",
    "        [cordex_output_path ,surfex_output_path]*4)\n",
    "\n",
    "    fig, axs = plt.subplots(4,2, figsize=(W, H), frameon=True, constrained_layout=False, sharey=True)\n",
    "    for ax,df_compound in zip (axs.flat, gen_cdx_sfx_ts_cmp_h_mean(models_seq=gen)):\n",
    "        df, model, season = df_compound\n",
    "        aux_plot_models_obs_hourly_mean(df, ax, title='{}: {}'.format(model, season))\n",
    "    # get legend form last axe\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    leg = fig.legend(handles, labels, ncol=4,frameon=False, loc=8, fontsize=14)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.1)                         # TRICK\n",
    "    \n",
    "    \n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)\n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cmp_metric_avg_h(metrics, data_ref, data_cmp, title_fig=None,\n",
    "                    out_filename=None, out_dir=plots_output_path, sub_folder=None ):    \n",
    "    '''\n",
    "        OBS1:\n",
    "            |avg(sigma)-1| or avg|sigma-1|?\n",
    "            Here we adopt avg|sigma-1|\n",
    "        OBS2:\n",
    "            |Bias|.avg() or |Bias.avg()|?\n",
    "            Adopted |Bias|.avg()\n",
    "        OBS3:\n",
    "            Computed the abs for all metrics: |Bias|,|Perkins|,|YK| and\n",
    "            |Sigma-1|\n",
    "    '''\n",
    "    # fig dimensions\n",
    "    W, H = 12, 12\n",
    "\n",
    "    from itertools import cycle\n",
    "    plot_id = cycle('abcdefghijklmnopqrstuvxwz')  \n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2,2, figsize=(W, H), frameon=True, constrained_layout=False)\n",
    "    axs = axs.flat\n",
    "    for metric,d_ref,d_cmp,ax in zip(metrics, data_ref, data_cmp, axs):\n",
    "        # TODO -> function\n",
    "        if metric == 'sigma':\n",
    "            d_ref = d_ref-1\n",
    "            d_cmp = d_cmp-1\n",
    "        # ABS\n",
    "        d_ref, d_cmp = d_ref.abs(), d_cmp.abs()\n",
    "        \n",
    "        data_x_min, data_x_max = d_ref.min(), d_ref.max()\n",
    "        data_y_min, data_y_max = d_cmp.min(), d_cmp.max()\n",
    "        x_min = min([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "        x_max = max([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "        if (x_max > 1) and (metric!='sigma'): x_max = np.ceil(x_max)                            # TODO BETTER\n",
    "\n",
    "        \n",
    "        data = pd.DataFrame()\n",
    "        data['CDX'] = d_ref\n",
    "        data['SFX'] = d_cmp\n",
    "        plot = aux_plot_cmp_metric_avg_h(metric, ax,\n",
    "                               'NOTHING',        # FIX IT TODO\n",
    "                               data,\n",
    "                               x_min, x_max,\n",
    "                               plot_id,)\n",
    "\n",
    "    # get legend form last axe\n",
    "#     handles, labels = ax.get_legend_handles_labels()\n",
    "#     leg = fig.legend(handles, labels, \n",
    "#                      ncol=3, mode='expand', \n",
    "#                      frameon=False, #loc=8,\n",
    "#                      fontsize=14, markerscale=1, bbox_to_anchor=(0.5, 0.15))\n",
    "    fig.suptitle('Season: {}'.format(title_fig), fontsize=14)\n",
    "#     fig.tight_layout()\n",
    "#     fig.subplots_adjust(top=0.95, bottom=0.1)                         # TRICK\n",
    "\n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "# def plot_cmp_metric_avg_h_OLD(metric, data_ref, data_cmp, title_fig=None,\n",
    "#                     out_filename=None, out_dir=plots_output_path, sub_folder=None ):\n",
    "#     '''\n",
    "#         OBS: Corrects for Sigma^-1\n",
    "#     '''\n",
    "#     # fig dimensions\n",
    "#     W, H = 12, 24\n",
    "    \n",
    "    \n",
    "#     if metric == 'sigma':\n",
    "#         data_ref = 1/data_ref\n",
    "#         data_cmp = 1/data_cmp\n",
    "\n",
    "#     data_x_min, data_x_max = data_ref.min().min(), data_ref.max().max()\n",
    "#     data_y_min, data_y_max = data_cmp.min().min(), data_cmp.max().max()\n",
    "#     x_min = min([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "#     x_max = max([data_x_min, data_x_max, data_y_min, data_y_max])\n",
    "#     #x_min = np.floor(x_min)\n",
    "#     if (x_max > 1) and (metric!='sigma'): x_max = np.ceil(x_max)                            # TODO BETTER\n",
    "\n",
    "#     from itertools import cycle\n",
    "#     plot_id = cycle('abcdefghijklmnopqrstuvxwz')  \n",
    "\n",
    "\n",
    "# #     hours = range(0,22,3)\n",
    "#     fig, axs = plt.subplots(2,2, figsize=(W, H), frameon=True, constrained_layout=False)\n",
    "#     axs = axs.flat\n",
    "#     for metric, ax in zip(hours, axs):\n",
    "#         data = pd.DataFrame()\n",
    "#         data['CDX'] = data_ref.loc[:,hour]\n",
    "#         data['SFX'] = data_cmp.loc[:,hour]\n",
    "#         plot = aux_plot_cmp_metric(metric, ax,\n",
    "#                                hour,\n",
    "#                                data,\n",
    "#                                x_min, x_max,\n",
    "#                                plot_id,)\n",
    "#                                #param_dict={'zorder':1})\n",
    "\n",
    "#     # get legend form last axe\n",
    "#     handles, labels = ax.get_legend_handles_labels()\n",
    "#     leg = fig.legend(handles, labels, \n",
    "#                      ncol=3, mode='expand', \n",
    "#                      frameon=False, loc=8,\n",
    "#                      fontsize=14, markerscale=1)\n",
    "#     fig.suptitle('Season: {}'.format(title_fig), fontsize=14)\n",
    "# #     fig.tight_layout()\n",
    "#     fig.subplots_adjust(top=0.95, bottom=0.1)                         # TRICK\n",
    "    \n",
    "#     if out_filename:\n",
    "#         if sub_folder:\n",
    "#             out_dir = os.path.join(out_dir, sub_folder)\n",
    "#         save_plot(plt, out_filename, output_dir = out_dir)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assorted tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_markers(scale_markers):\n",
    "    '''\n",
    "        Function to invert the clustering of the markers to the regional name.\n",
    "        scale_markers: dict made for the scale markers\n",
    "    '''\n",
    "    from itertools import chain\n",
    "    del models_markers[18]\n",
    "    del models_markers[19]\n",
    "    regional_markers = map(lambda s: (s[0],'_'.join(s[1][0].split('_')[::-1])), models_markers.items())\n",
    "    regional_markers = sorted(regional_markers, key=lambda i: i[1])\n",
    "    regional_markers = zip(regional_markers,chain('d', '1'*2, '+'*4, '^', '*'*2, '2'*2, 'o'*5))\n",
    "    return {pair[0]:(pair[1],marker)for pair, marker in regional_markers}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_diurnal_bias(bias_probe):\n",
    "    avg_diurnal = pd.DataFrame(bias_probe, index=range(0,22,3))\n",
    "    return pd.DataFrame(avg_diurnal[17]-avg_diurnal.iloc[:,:-1].mean(axis=1),\n",
    "                       columns=[19])\n",
    "\n",
    "def avg_diurnal_sigma(sigma_probe):\n",
    "    avg_diurnal = pd.DataFrame(sigma_probe, index=range(0,22,3))\n",
    "    return pd.DataFrame(avg_diurnal.iloc[:,:-1].mean(axis=1)/avg_diurnal[17],\n",
    "                        columns=[19])\n",
    "\n",
    "def avg_diurnal_perkins(perkins_probe):\n",
    "    list_S = []\n",
    "    for df_pdfs in perkins_probe:\n",
    "        df_pdfs[ens_alpha_col] = df_pdfs.iloc[:,:-1].mean(axis=1)\n",
    "        S = fn_models_obs(minimun, df_pdfs, [ens_alpha_col],\n",
    "                          df_pdfs[temp_obs_col]).sum(axis=0)*100\n",
    "        list_S.append(S.values)\n",
    "    return pd.DataFrame(list_S, index=range(0,22,3), columns=[19])\n",
    "\n",
    "def avg_diurnal_yk(yk_probe):\n",
    "    list_yk = []\n",
    "    for df_perc in yk_probe:\n",
    "        df_perc[ens_alpha_col] = df_perc.iloc[:,:-1].mean(axis=1)\n",
    "        list_yk.append(yk_perc(df_perc.loc[:,[temp_obs_col,ens_alpha_col]]))\n",
    "    df_yk = pd.DataFrame(pd.concat(list_yk, axis=1).T)\n",
    "    df_yk = df_yk[ens_alpha_col] - df_yk[temp_obs_col]\n",
    "    return pd.DataFrame(df_yk.values, index=range(0,22,3), columns=[19])\n",
    "\n",
    "\n",
    "def metrics_models_ensemblesALPHA(data_file, season, data_input_path, family_models,\n",
    "                                 models_columns, obs_columns, ensemble_column,\n",
    "                                 plot=False, sub_folder=None):\n",
    "    '''\n",
    "    Only works for Ensemble BETA: using all models' data points, with equal weights, as one single model.\n",
    "    The problem is that it doesn't consider the model climate as a characteristic!\n",
    "    '''\n",
    "    # Load models and observations\n",
    "    df_all_sea = load_csv2df(data_file, data_input_path, freq_index='3H')\n",
    "    \n",
    "    \n",
    "    # Only the probes are used (probes give access to intermediate results)\n",
    "    # BIAS\n",
    "    df_bias_sea, bias_probe = metric_by_hour(bias, df_all_sea, models_columns, obs_columns)\n",
    "    df_bias_ens_alpha = avg_diurnal_bias(bias_probe)\n",
    "    df_bias_sea = pd.concat([df_bias_sea, df_bias_ens_alpha.T])\n",
    "    \n",
    "\n",
    "    # SIGMA\n",
    "    # Calculate metrics for bare models\n",
    "    df_sigma_sea, sigma_probe = metric_by_hour(sigma_score, df_all_sea, models_columns, obs_columns)\n",
    "    df_sigma_ens_alpha = avg_diurnal_sigma(sigma_probe)\n",
    "    df_sigma_sea = pd.concat([df_sigma_sea, df_sigma_ens_alpha.T])\n",
    "\n",
    "\n",
    "    # PERKINS SKILL SCORE\n",
    "    # Calculate metrics for bare models\n",
    "    df_perkins_sea, perkins_probe = metric_by_hour(perkins_skill_score, df_all_sea, models_columns, obs_columns)\n",
    "    df_perkins_ens_alpha = avg_diurnal_perkins(perkins_probe)\n",
    "    df_perkins_sea = pd.concat([df_perkins_sea, df_perkins_ens_alpha.T])\n",
    "\n",
    "    # YK SKEWNESS\n",
    "    # Calculate metrics for bare models\n",
    "    df_yk_sea, yk_probe = YK_skewness_by_hour(df_all_sea, models_columns, obs_columns)\n",
    "    df_yk_ens_alpha = avg_diurnal_yk(yk_probe)\n",
    "    df_yk_sea = pd.concat([df_yk_sea, df_yk_ens_alpha.T])\n",
    "\n",
    "    if plot:\n",
    "        plt_metrics_by_hour(df_bias_sea, '{}: Bias for 17 {} + Ensemble_ALPHA models by hour'.format(season, family_models),\n",
    "                            filename='bias_{}_{}_ensALPHA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_sigma_sea, '{}: Sigma score for 17 {} + Ensemble_ALPHA models by hour'.format(season, family_models),\n",
    "                            filename='sigma_{}_{}_ensALPHA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_perkins_sea, '{}: Perkins Skill score for 17 {} + Ensemble_ALPHA models by hour'.format(season, family_models),\n",
    "                            filename='perkins_{}_{}_ensALPHA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_yk_sea, '{}: Yule-Kendall skewness for 17 {} + Ensemble_ALPHA models by hour'.format(season, family_models),\n",
    "                            filename='yk_{}_{}_ensALPHA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "\n",
    "    return df_bias_sea, df_sigma_sea, df_perkins_sea, df_yk_sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_models_ensemblesBETA(data_file, season, data_input_path, family_models,\n",
    "                                 models_columns, obs_columns, ensemble_column,\n",
    "                                 plot=False, sub_folder=None):\n",
    "    '''\n",
    "    Only works for Ensemble BETA: using all models' data points, with equal weights, as one single model.\n",
    "    The problem is that it doesn't consider the model climate as a characteristic!\n",
    "    \n",
    "    Uses metric_by_hour() to compute metrics and probes (not used for ensemble BETA)\n",
    "    Can plot using plt_metrics_by_hour()\n",
    "    \n",
    "    \n",
    "    Returns: df_bias_sea, df_sigma_sea, df_perkins_sea, df_yk_sea\n",
    "        values: metric\n",
    "        cols: hours\n",
    "        rows: models + ensemble\n",
    "    \n",
    "    '''\n",
    "    # Load models and observations\n",
    "    df_all_sea = load_csv2df(data_file, data_input_path, freq_index='3H')\n",
    "\n",
    "    # Prepare df for ensemble calculations - BETA\n",
    "    df_obs_sea, df_models_sea = df_all_sea[[obs_columns]], df_all_sea[models_columns]\n",
    "    df_models_sea = df_models_sea.stack().to_frame(name=ensemble_column)\n",
    "    df_models_sea.reset_index(level=1, drop=True, inplace=True)\n",
    "    df_all_sea_ens = df_models_sea.join(df_obs_sea, how='left')\n",
    "    df_all_sea_ens.head()\n",
    "\n",
    "    # BIAS\n",
    "    # Calculate metrics for bare models\n",
    "    df_bias_sea, _ = metric_by_hour(bias, df_all_sea, models_columns, obs_columns)\n",
    "    # Calculate bias for ensemble\n",
    "    df_bias_sea_ens, _ = metric_by_hour(bias, df_all_sea_ens, [ensemble_column], obs_columns)\n",
    "    # append\n",
    "    df_bias_sea = df_bias_sea.append(df_bias_sea_ens)\n",
    "\n",
    "    # SIGMA\n",
    "    # Calculate metrics for bare models\n",
    "    df_sigma_sea, _ = metric_by_hour(sigma_score, df_all_sea, models_columns, obs_columns)\n",
    "    # Calculate bias for ensemble\n",
    "    df_sigma_sea_ens, _ = metric_by_hour(sigma_score, df_all_sea_ens, [ensemble_column], obs_columns)\n",
    "    # append\n",
    "    df_sigma_sea = df_sigma_sea.append(df_sigma_sea_ens)\n",
    "\n",
    "    # PERKINS SKILL SCORE\n",
    "    # Calculate metrics for bare models\n",
    "    df_perkins_sea, _ = metric_by_hour(perkins_skill_score, df_all_sea, models_columns, obs_columns)\n",
    "    # Calculate bias for ensemble\n",
    "    df_perkins_sea_ens, _ = metric_by_hour(perkins_skill_score, df_all_sea_ens, [ensemble_column], obs_columns)\n",
    "    # append\n",
    "    df_perkins_sea = df_perkins_sea.append(df_perkins_sea_ens)\n",
    "\n",
    "    # YK SKEWNESS\n",
    "    # Calculate metrics for bare models\n",
    "    df_yk_sea, _ = YK_skewness_by_hour(df_all_sea, models_columns, obs_columns)\n",
    "    # Calculate bias for ensemble\n",
    "    df_yk_sea_ens, _ = YK_skewness_by_hour(df_all_sea_ens, [ensemble_column], obs_columns)\n",
    "    # append\n",
    "    df_yk_sea = df_yk_sea.append(df_yk_sea_ens)\n",
    "\n",
    "    if plot:\n",
    "        plt_metrics_by_hour(df_bias_sea, '{}: Bias for 17 {} + Ensemble_BETA models by hour'.format(season, family_models),\n",
    "                            filename='bias_{}_{}_ensBETA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_sigma_sea, '{}: Sigma score for 17 {} + Ensemble_BETA models by hour'.format(season, family_models),\n",
    "                            filename='sigma_{}_{}_ensBETA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_perkins_sea, '{}: Perkins Skill score for 17 {} + Ensemble_BETA models by hour'.format(season, family_models),\n",
    "                            filename='perkins_{}_{}_ensBETA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "        plt_metrics_by_hour(df_yk_sea, '{}: Yule-Kendall skewness for 17 {} + Ensemble_BETA models by hour'.format(season, family_models),\n",
    "                            filename='yk_{}_{}_ensBETA'.format(season.lower(),family_models.lower()), sub_folder = sub_folder)\n",
    "\n",
    "    return df_bias_sea, df_sigma_sea, df_perkins_sea, df_yk_sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_plot_cmp_metric_avg_h(metric, ax, hour, data, d_min, d_max, id_letter):#, param_dict):\n",
    "    \"\"\"\n",
    "    A helper function to make a metric comparison graph\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : Axes. The axes to draw to\n",
    "    data : Dataframe. The data as a df with 2 cols\n",
    "    d_min: the floor.min of all data points\n",
    "    d_max: the ceil.max of all data points\n",
    "    id_letter: letters to identify the subplots\n",
    "    #param_dict : dict. Dictionary of kwargs to pass to ax.plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        list of artists added\n",
    "    \"\"\"\n",
    "    metric = metric.lower()\n",
    "#     metrics_dict = {'bias': '|Bias|', 'sigma': r'|$\\sigma_n^{-1}$|',    # TODO - mov to configurations file\n",
    "#                     'perkins': '|S|', 's': '|S|', 'yk': '|YK|'}\n",
    "    for idx,row in data.iterrows():\n",
    "        out = ax.scatter(row[0], row[1], \n",
    "                         label=models_markers[idx][0], \n",
    "                         marker=models_markers[idx][1],\n",
    "                         s=100,)\n",
    "                        # **param_dict)\n",
    "    if metric in ['bias', 'sigma', 'yk']:\n",
    "        ax.fill([d_min, d_min, d_max, d_min], [d_min, d_max, d_max, d_min], fill=True, alpha=0.3, color='lightgray')\n",
    "    elif metric in ['perkins', 's']: # only for perkins\n",
    "        ax.fill([d_min, d_max, d_max, d_min], [d_min, d_min, d_max, d_min], fill=True, alpha=0.3, color='lightgray')\n",
    "    else:\n",
    "        print('Metric not allowed! Chose among: bias, perkins, sigma, yk')\n",
    "    \n",
    "    ax.set_xlabel(' '.join(('CORDEX {}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "    ax.set_ylabel(' '.join(('SFX {}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "    \n",
    "    # outer limits\n",
    "    dd_min, dd_max= d_min-0.05*d_max, d_max+0.05*d_max\n",
    "    ax.set_xlim(dd_min, dd_max)\n",
    "    ax.set_ylim(dd_min, dd_max)\n",
    "    ax.text(-0.15, 0.95, '{})'.format(next(id_letter)),\n",
    "            fontsize= 16, transform=ax.transAxes)\n",
    "#     ax.text(0.5, 0.95, r'$={}h$'.format(),\n",
    "#         horizontalalignment='center', fontsize=14, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def prepare_data_4x4(metrics_function, lst_data_files, input_path, model_family, models_columns, obs_columns, ensemble_column):\n",
    "    from itertools import chain\n",
    "    \n",
    "    seasons_list = list(SEASONS)\n",
    "    all_sea_all_met = []\n",
    "    for file,sea in zip(lst_data_files, seasons_list):\n",
    "#         lst_df = map(abs, metrics_function(file, sea, input_path, model_family, models_columns, obs_columns, ensemble_column))\n",
    "        lst_df = metrics_function(file, sea, input_path, model_family, models_columns, obs_columns, ensemble_column)\n",
    "        all_sea_all_met.append(lst_df)\n",
    "    return chain(*zip(*all_sea_all_met))\n",
    "\n",
    "        \n",
    "        \n",
    "def plot_cmp_metric_seasons_h_ensembles(data_ref, data_cmp, ensemble_column,\n",
    "                                        title_fig=None, out_filename=None, out_dir=None, sub_folder=None ): \n",
    "    '''\n",
    "        OBS:\n",
    "            Computed the abs for all metrics: |Bias|,|Perkins|,|YK| and\n",
    "            |Sigma-1|\n",
    "    '''\n",
    "    \n",
    "    from itertools import cycle, chain\n",
    "    \n",
    "    # id each plot with a letter\n",
    "    plot_id = cycle('abcdefghijklmnopqrstuvxwz')\n",
    "    W, H = 18, 18\n",
    "    # separate the 2 columns of points in x axis: (x-delta) for CORDEX; (x+delta) for SURFEX \n",
    "    delta = 0.45\n",
    "    # produce the list: ['bias', 'bias', 'bias', 'bias', 'sigma',...,'sigma',..,'perkins', 'yk', 'yk', 'yk', 'yk']\n",
    "    metrics = list(chain(*([metric]*4 for metric in METRIC_NAMES)))\n",
    "    seasons_list_4 = list(SEASONS)*4\n",
    "\n",
    "\n",
    "    #PLOTTING\n",
    "    fig, axs = plt.subplots(4,4, figsize=(W, H), frameon=True, sharey='row') \n",
    "    axs = axs.flat\n",
    "    for i,tup in enumerate(zip(seasons_list_4, metrics, axs, data_ref, data_cmp)):  # i: to implement cond. on the labels\n",
    "#     for i,tup in enumerate(zip(seasons_list_4, metrics, axs, df_cdx, df_sfx)):  # i: to implement cond. on the labels        \n",
    "        season,metric,ax,cdx,sfx = tup\n",
    "        # cols: models; rows: hours -  to use pandas plot of a df\n",
    "        cdx, sfx = cdx.copy().T, sfx.copy().T\n",
    "        if metric == 'sigma':\n",
    "            cdx = cdx-1\n",
    "            sfx = sfx-1\n",
    "        # ABS\n",
    "        cdx, sfx = cdx.abs(), sfx.abs()\n",
    "        \n",
    "        # (x-delta)\n",
    "        cdx = cdx.reset_index().rename({'index':'hour'}, axis=1)\n",
    "        cdx['hour'] = cdx['hour']-delta\n",
    "        # (x+delta)\n",
    "        sfx = sfx.reset_index().rename({'index':'hour'}, axis=1)\n",
    "        sfx['hour'] = sfx['hour']+delta\n",
    "        # plot each column (model) except the 'hour' column\n",
    "        for col in cdx.iloc[:,1:]:\n",
    "            cdx.plot(x='hour',  y=col, kind='scatter', label='CORDEX', color = 'r', marker='.', ax=ax)\n",
    "            sfx.plot(x='hour',  y=col, kind='scatter', label='SURFEX', color='c', marker= '.', ax=ax) \n",
    "        # plot the ensembles\n",
    "        cdx.plot(x='hour',  y=ensemble_column, kind='scatter', label='Ensemble - CORDEX', color='k', marker='$C$', s=60, ax=ax)\n",
    "        sfx.plot(x='hour',  y=ensemble_column, kind='scatter', label='Ensemble - SURFEX', color='k',  marker='$S$', s=60, ax=ax) \n",
    "        ax.set_xticks(list(range(0,22,3)))    \n",
    "\n",
    "        # ylabel only for the left column\n",
    "        if i in [0,4,8,12]:\n",
    "            ax.set_ylabel(' '.join(('{}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "        else:\n",
    "            ax.set_ylabel(None)\n",
    "        # xlabel only fo thr bottom row\n",
    "        if i >= 12:\n",
    "            ax.set_xlabel('hour')\n",
    "        else:\n",
    "            ax.set_xlabel(None)\n",
    "        # letter id for all plots\n",
    "        ax.text(-0.15, 1.05, '{})'.format(next(plot_id)),\n",
    "                fontsize= 14, transform=ax.transAxes)\n",
    "        # season name for top row\n",
    "        if i in [0,1,2,3]:\n",
    "            ax.text(0.5, 1.05, r'{}'.format(season),\n",
    "                horizontalalignment='center', fontsize=14, ha='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            pass\n",
    "        ax.get_legend().remove()                       # pandas by default draws the legend   \n",
    "\n",
    "    # get legend form last axe and make 2 separted legends so the first one has the scatterpoints=4\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # make the legend for CORDEX and SURFEX\n",
    "    _ = fig.legend(handles[-4:-2], labels[-4:-2],  ncol=2, frameon=True, loc=8, fontsize=14, markerscale=2, # scales marker relative to plot\n",
    "                   bbox_to_anchor=(0.35, 0.05),           # controls the position\n",
    "                   bbox_transform=plt.gcf().transFigure,  # coords refrenced to the figure\n",
    "                   scatterpoints=4)\n",
    "    # make the second legend for the ensembles\n",
    "    _ = fig.legend(handles[-2:], labels[-2:], ncol=2, frameon=True, loc=8, fontsize=14,\n",
    "                         bbox_to_anchor=(0.65, 0.05),\n",
    "                         bbox_transform=plt.gcf().transFigure)\n",
    "    if not title_fig:\n",
    "        title_fig = r'T$_{2m}$ Error metrics for CORDEX and SURFEX models and its respective Ensembles.'\n",
    "    fig.suptitle(title_fig, fontsize=14)\n",
    "    fig.subplots_adjust(top=0.93, bottom=0.1)\n",
    "#     fig.tight_layout() \n",
    "    \n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)\n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_cmp_metric_seasons_h_ensembles(data_ref, data_cmp, ensemble_column,\n",
    "                                        title_fig=None, out_filename=None, out_dir=None, sub_folder=None ): \n",
    "    '''\n",
    "        OBS:\n",
    "            Computed the abs for all metrics: |Bias|,|Perkins|,|YK| and\n",
    "            |Sigma-1|\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    from itertools import cycle, chain\n",
    "    from matplotlib.patches import Polygon\n",
    "    \n",
    "    def separate_ens_data(data_arr):\n",
    "        new_data, ens_data = [], []\n",
    "        for item in data_arr:\n",
    "            ens_data.append(item[-1])\n",
    "            item = np.delete(item, -1, 0)\n",
    "            new_data.append(item)\n",
    "        return new_data, np.array(ens_data) \n",
    "    \n",
    "    # id each plot with a letter\n",
    "    plot_id = cycle('abcdefghijklmnopqrstuvxwz')\n",
    "    W, H = 18, 18\n",
    "    # produce the list: ['bias', 'bias', 'bias', 'bias', 'sigma',...,'sigma',..,'perkins', 'yk', 'yk', 'yk', 'yk']\n",
    "    metrics = list(chain(*([metric]*4 for metric in METRIC_NAMES)))\n",
    "    seasons_list_4 = list(SEASONS)*4\n",
    "    color_cdx, color_sfx = 'c','y'#'lightcyan', 'lightsalmon'\n",
    "\n",
    "    #PLOTTING\n",
    "    fig, axs = plt.subplots(4,4, figsize=(W, H), frameon=True, sharey='row') \n",
    "    axs = axs.flat\n",
    "    for i,tup in enumerate(zip(seasons_list_4, metrics, axs, data_ref, data_cmp)):  # i: to implement cond. on the labels\n",
    "        season,metric,ax,cdx,sfx = tup\n",
    "        # cols: models; rows: hours -  to use pandas plot of a df\n",
    "        cdx, sfx = cdx.copy(), sfx.copy()\n",
    "        \n",
    "        if metric == 'sigma':\n",
    "            cdx = cdx - 1\n",
    "            sfx = sfx - 1\n",
    "        # ABS\n",
    "        cdx, sfx = cdx.abs(), sfx.abs()\n",
    "        \n",
    "        data = list(chain(*zip(cdx.values.T, sfx.values.T)))\n",
    "        data, ensemble = separate_ens_data(data)\n",
    "#         print(season,metric, data, ensemble)\n",
    "        \n",
    "        bp = ax.boxplot(data, notch=0, sym='+', vert=1, whis=1.5)\n",
    "        \n",
    "        plt.setp(bp['boxes'], color='black')\n",
    "        plt.setp(bp['whiskers'], color='black')\n",
    "        plt.setp(bp['medians'], color='black')\n",
    "        plt.setp(bp['fliers'], color='black', marker='.')\n",
    "        \n",
    "        # the first xtick is 0 we have to put a dummy number there\n",
    "        head_0 = np.ma.masked_where(1, [0], copy=True)\n",
    "        cdx_mask = np.ma.masked_where(np.arange(0,16)%2, ensemble, copy=True)\n",
    "        cdx_mask = np.ma.append(head_0,cdx_mask)\n",
    "        sfx_mask = np.ma.masked_where(~np.arange(0,16)%2, ensemble, copy=True)\n",
    "        sfx_mask = np.ma.append(head_0,sfx_mask)\n",
    "        ax.plot(cdx_mask, color='k', marker='$C$', label='Ensemble - CORDEX')\n",
    "        ax.plot(sfx_mask, color='k', marker='$S$', label='Ensemble - SURFEX')\n",
    "\n",
    "        # Add a horizontal grid to the plot, but make it very light in color\n",
    "        # so we can use it for reading data values but not be distracting\n",
    "        ax.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
    "                       alpha=0.5)\n",
    "\n",
    "        # Now fill the boxes with desired colors\n",
    "        box_colors = [color_cdx, color_sfx]\n",
    "        num_boxes = len(data)\n",
    "        medians = np.empty(num_boxes)\n",
    "        for j in range(num_boxes):\n",
    "            box = bp['boxes'][j]\n",
    "            box_x, box_y = [], []\n",
    "            # dict_keys(['whiskers', 'caps', 'boxes', 'medians', 'fliers', 'means'])\n",
    "            for m in range(5): \n",
    "                box_x.append(box.get_xdata()[m])\n",
    "                box_y.append(box.get_ydata()[m])\n",
    "            box_coords = np.column_stack([box_x, box_y])\n",
    "            # Alternate between color_cdx and color_sfx\n",
    "            ax.add_patch(Polygon(box_coords, facecolor=box_colors[j % 2]))\n",
    "\n",
    "        # delete tick labels\n",
    "        ax.set_xticklabels(['' for i in range(17)])\n",
    "        # re-make the tick labels\n",
    "        pos = np.arange(num_boxes) + 1\n",
    "        labels = np.repeat(range(0,22,3), 2)\n",
    "        for tick, label in zip(range(num_boxes), ax.get_xticklabels()):\n",
    "            k = tick % 2\n",
    "            ax.text(pos[tick]+0.5, -0.05, labels[tick],\n",
    "                 transform=ax.get_xaxis_transform(),\n",
    "                 horizontalalignment='center',\n",
    "                 color='w' if k else 'k')\n",
    "\n",
    "        # ylabel only for the left column\n",
    "        if i in [0,4,8,12]:\n",
    "            ax.set_ylabel(' '.join(('{}'.format(metrics_symbols[metric]), r'T$_{2m}$ $^{o}$C')), fontsize=14)\n",
    "        else:\n",
    "            ax.set_ylabel(None)\n",
    "        # xlabel only fo thr bottom row\n",
    "        if i >= 12:\n",
    "            ax.set_xlabel('hour')\n",
    "        else:\n",
    "            ax.set_xlabel(None)\n",
    "        # letter id for all plots\n",
    "        ax.text(-0.15, 1.05, '{})'.format(next(plot_id)),\n",
    "                fontsize= 14, transform=ax.transAxes)\n",
    "        # season name for top row\n",
    "        if i in [0,1,2,3]:\n",
    "            ax.text(0.5, 1.05, r'{}'.format(season),\n",
    "                horizontalalignment='center', fontsize=14, ha='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if not title_fig:\n",
    "        title_fig = r'T$_{2m}$ Error metrics for CORDEX and SURFEX models and its respective Ensembles.'\n",
    "    fig.suptitle(title_fig, fontsize=14)\n",
    "    fig.subplots_adjust(top=0.93, bottom=0.05)\n",
    "    \n",
    "\n",
    "    # Finally, add a basic legend\n",
    "    fig.text(0.30, 0.015, 'CORDEX',\n",
    "             backgroundcolor=box_colors[0], color='black')#, size='x-small')\n",
    "    fig.text(0.37, 0.015, 'SURFEX',\n",
    "             backgroundcolor=box_colors[1], color='black')#, size='x-small')\n",
    "    fig.text(0.45, 0.015, 'C Esemble-CORDEX', color='black')#, size='x-small')\n",
    "    fig.text(0.55, 0.015, 'S Ensemble-SURFEX', color='black')#, size='x-small')\n",
    "    \n",
    "    if out_filename:\n",
    "        if sub_folder:\n",
    "            out_dir = os.path.join(out_dir, sub_folder)\n",
    "        save_plot(plt, out_filename, output_dir = out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
